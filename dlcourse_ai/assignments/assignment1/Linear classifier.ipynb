{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "linear_classifer.softmax_with_cross_entropy(predictions, target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 4\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "loss\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.484536\n",
      "Epoch 1, loss: 2.356184\n",
      "Epoch 2, loss: 2.318042\n",
      "Epoch 3, loss: 2.306638\n",
      "Epoch 4, loss: 2.303287\n",
      "Epoch 5, loss: 2.302302\n",
      "Epoch 6, loss: 2.301991\n",
      "Epoch 7, loss: 2.301892\n",
      "Epoch 8, loss: 2.301875\n",
      "Epoch 9, loss: 2.301859\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa6225b6048>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.082\n",
      "Epoch 0, loss: 2.301880\n",
      "Epoch 1, loss: 2.301869\n",
      "Epoch 2, loss: 2.301865\n",
      "Epoch 3, loss: 2.301857\n",
      "Epoch 4, loss: 2.301859\n",
      "Epoch 5, loss: 2.301884\n",
      "Epoch 6, loss: 2.301865\n",
      "Epoch 7, loss: 2.301863\n",
      "Epoch 8, loss: 2.301884\n",
      "Epoch 9, loss: 2.301863\n",
      "Epoch 10, loss: 2.301852\n",
      "Epoch 11, loss: 2.301859\n",
      "Epoch 12, loss: 2.301846\n",
      "Epoch 13, loss: 2.301862\n",
      "Epoch 14, loss: 2.301862\n",
      "Epoch 15, loss: 2.301889\n",
      "Epoch 16, loss: 2.301865\n",
      "Epoch 17, loss: 2.301863\n",
      "Epoch 18, loss: 2.301874\n",
      "Epoch 19, loss: 2.301865\n",
      "Epoch 20, loss: 2.301849\n",
      "Epoch 21, loss: 2.301864\n",
      "Epoch 22, loss: 2.301849\n",
      "Epoch 23, loss: 2.301870\n",
      "Epoch 24, loss: 2.301869\n",
      "Epoch 25, loss: 2.301857\n",
      "Epoch 26, loss: 2.301877\n",
      "Epoch 27, loss: 2.301867\n",
      "Epoch 28, loss: 2.301849\n",
      "Epoch 29, loss: 2.301854\n",
      "Epoch 30, loss: 2.301862\n",
      "Epoch 31, loss: 2.301862\n",
      "Epoch 32, loss: 2.301853\n",
      "Epoch 33, loss: 2.301877\n",
      "Epoch 34, loss: 2.301866\n",
      "Epoch 35, loss: 2.301865\n",
      "Epoch 36, loss: 2.301858\n",
      "Epoch 37, loss: 2.301851\n",
      "Epoch 38, loss: 2.301872\n",
      "Epoch 39, loss: 2.301859\n",
      "Epoch 40, loss: 2.301866\n",
      "Epoch 41, loss: 2.301858\n",
      "Epoch 42, loss: 2.301863\n",
      "Epoch 43, loss: 2.301867\n",
      "Epoch 44, loss: 2.301864\n",
      "Epoch 45, loss: 2.301872\n",
      "Epoch 46, loss: 2.301868\n",
      "Epoch 47, loss: 2.301883\n",
      "Epoch 48, loss: 2.301870\n",
      "Epoch 49, loss: 2.301858\n",
      "Epoch 50, loss: 2.301856\n",
      "Epoch 51, loss: 2.301865\n",
      "Epoch 52, loss: 2.301870\n",
      "Epoch 53, loss: 2.301870\n",
      "Epoch 54, loss: 2.301863\n",
      "Epoch 55, loss: 2.301847\n",
      "Epoch 56, loss: 2.301873\n",
      "Epoch 57, loss: 2.301872\n",
      "Epoch 58, loss: 2.301868\n",
      "Epoch 59, loss: 2.301851\n",
      "Epoch 60, loss: 2.301860\n",
      "Epoch 61, loss: 2.301859\n",
      "Epoch 62, loss: 2.301867\n",
      "Epoch 63, loss: 2.301865\n",
      "Epoch 64, loss: 2.301871\n",
      "Epoch 65, loss: 2.301869\n",
      "Epoch 66, loss: 2.301848\n",
      "Epoch 67, loss: 2.301860\n",
      "Epoch 68, loss: 2.301864\n",
      "Epoch 69, loss: 2.301860\n",
      "Epoch 70, loss: 2.301864\n",
      "Epoch 71, loss: 2.301873\n",
      "Epoch 72, loss: 2.301851\n",
      "Epoch 73, loss: 2.301871\n",
      "Epoch 74, loss: 2.301889\n",
      "Epoch 75, loss: 2.301869\n",
      "Epoch 76, loss: 2.301877\n",
      "Epoch 77, loss: 2.301871\n",
      "Epoch 78, loss: 2.301864\n",
      "Epoch 79, loss: 2.301866\n",
      "Epoch 80, loss: 2.301871\n",
      "Epoch 81, loss: 2.301868\n",
      "Epoch 82, loss: 2.301862\n",
      "Epoch 83, loss: 2.301852\n",
      "Epoch 84, loss: 2.301876\n",
      "Epoch 85, loss: 2.301856\n",
      "Epoch 86, loss: 2.301868\n",
      "Epoch 87, loss: 2.301865\n",
      "Epoch 88, loss: 2.301863\n",
      "Epoch 89, loss: 2.301878\n",
      "Epoch 90, loss: 2.301864\n",
      "Epoch 91, loss: 2.301873\n",
      "Epoch 92, loss: 2.301861\n",
      "Epoch 93, loss: 2.301872\n",
      "Epoch 94, loss: 2.301884\n",
      "Epoch 95, loss: 2.301877\n",
      "Epoch 96, loss: 2.301870\n",
      "Epoch 97, loss: 2.301869\n",
      "Epoch 98, loss: 2.301898\n",
      "Epoch 99, loss: 2.301862\n",
      "Accuracy after training for 100 epochs:  0.079\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.283578\n",
      "Epoch 1, loss: 2.221806\n",
      "Epoch 2, loss: 2.194549\n",
      "Epoch 3, loss: 2.182822\n",
      "Epoch 4, loss: 2.177756\n",
      "Epoch 5, loss: 2.159567\n",
      "Epoch 6, loss: 2.150808\n",
      "Epoch 7, loss: 2.153043\n",
      "Epoch 8, loss: 2.138009\n",
      "Epoch 9, loss: 2.139993\n",
      "Epoch 10, loss: 2.134289\n",
      "Epoch 11, loss: 2.132416\n",
      "Epoch 12, loss: 2.124453\n",
      "Epoch 13, loss: 2.123696\n",
      "Epoch 14, loss: 2.119438\n",
      "Epoch 15, loss: 2.113664\n",
      "Epoch 16, loss: 2.108598\n",
      "Epoch 17, loss: 2.104463\n",
      "Epoch 18, loss: 2.108194\n",
      "Epoch 19, loss: 2.109901\n",
      "Epoch 20, loss: 2.098200\n",
      "Epoch 21, loss: 2.099888\n",
      "Epoch 22, loss: 2.097741\n",
      "Epoch 23, loss: 2.091419\n",
      "Epoch 24, loss: 2.093511\n",
      "Epoch 25, loss: 2.088126\n",
      "Epoch 26, loss: 2.092232\n",
      "Epoch 27, loss: 2.081719\n",
      "Epoch 28, loss: 2.080612\n",
      "Epoch 29, loss: 2.082214\n",
      "Epoch 30, loss: 2.077747\n",
      "Epoch 31, loss: 2.076864\n",
      "Epoch 32, loss: 2.071582\n",
      "Epoch 33, loss: 2.073660\n",
      "Epoch 34, loss: 2.071093\n",
      "Epoch 35, loss: 2.069950\n",
      "Epoch 36, loss: 2.062306\n",
      "Epoch 37, loss: 2.066627\n",
      "Epoch 38, loss: 2.064298\n",
      "Epoch 39, loss: 2.063497\n",
      "Epoch 40, loss: 2.056478\n",
      "Epoch 41, loss: 2.055691\n",
      "Epoch 42, loss: 2.057780\n",
      "Epoch 43, loss: 2.056562\n",
      "Epoch 44, loss: 2.058397\n",
      "Epoch 45, loss: 2.052219\n",
      "Epoch 46, loss: 2.052189\n",
      "Epoch 47, loss: 2.054489\n",
      "Epoch 48, loss: 2.045089\n",
      "Epoch 49, loss: 2.048828\n",
      "Epoch 50, loss: 2.046363\n",
      "Epoch 51, loss: 2.048040\n",
      "Epoch 52, loss: 2.047610\n",
      "Epoch 53, loss: 2.047602\n",
      "Epoch 54, loss: 2.047632\n",
      "Epoch 55, loss: 2.044238\n",
      "Epoch 56, loss: 2.033391\n",
      "Epoch 57, loss: 2.038013\n",
      "Epoch 58, loss: 2.036938\n",
      "Epoch 59, loss: 2.038520\n",
      "Epoch 60, loss: 2.030443\n",
      "Epoch 61, loss: 2.035546\n",
      "Epoch 62, loss: 2.034265\n",
      "Epoch 63, loss: 2.028025\n",
      "Epoch 64, loss: 2.037159\n",
      "Epoch 65, loss: 2.034367\n",
      "Epoch 66, loss: 2.031449\n",
      "Epoch 67, loss: 2.027723\n",
      "Epoch 68, loss: 2.022670\n",
      "Epoch 69, loss: 2.027800\n",
      "Epoch 70, loss: 2.027918\n",
      "Epoch 71, loss: 2.020774\n",
      "Epoch 72, loss: 2.017107\n",
      "Epoch 73, loss: 2.013507\n",
      "Epoch 74, loss: 2.022603\n",
      "Epoch 75, loss: 2.019335\n",
      "Epoch 76, loss: 2.014669\n",
      "Epoch 77, loss: 2.022679\n",
      "Epoch 78, loss: 2.015822\n",
      "Epoch 79, loss: 2.021368\n",
      "Epoch 80, loss: 2.010380\n",
      "Epoch 81, loss: 2.006189\n",
      "Epoch 82, loss: 2.009174\n",
      "Epoch 83, loss: 2.011914\n",
      "Epoch 84, loss: 2.012268\n",
      "Epoch 85, loss: 2.012802\n",
      "Epoch 86, loss: 2.006109\n",
      "Epoch 87, loss: 2.010785\n",
      "Epoch 88, loss: 2.004636\n",
      "Epoch 89, loss: 2.010870\n",
      "Epoch 90, loss: 1.999590\n",
      "Epoch 91, loss: 2.003638\n",
      "Epoch 92, loss: 2.005616\n",
      "Epoch 93, loss: 2.002904\n",
      "Epoch 94, loss: 2.001769\n",
      "Epoch 95, loss: 1.998543\n",
      "Epoch 96, loss: 1.999599\n",
      "Epoch 97, loss: 1.999578\n",
      "Epoch 98, loss: 2.000701\n",
      "Epoch 99, loss: 1.996658\n",
      "Epoch 100, loss: 1.996858\n",
      "Epoch 101, loss: 1.996057\n",
      "Epoch 102, loss: 1.996847\n",
      "Epoch 103, loss: 1.996870\n",
      "Epoch 104, loss: 1.996785\n",
      "Epoch 105, loss: 1.996900\n",
      "Epoch 106, loss: 1.989524\n",
      "Epoch 107, loss: 1.993299\n",
      "Epoch 108, loss: 1.987516\n",
      "Epoch 109, loss: 1.990815\n",
      "Epoch 110, loss: 1.988926\n",
      "Epoch 111, loss: 1.990564\n",
      "Epoch 112, loss: 1.987216\n",
      "Epoch 113, loss: 1.984362\n",
      "Epoch 114, loss: 1.982030\n",
      "Epoch 115, loss: 1.983555\n",
      "Epoch 116, loss: 1.988831\n",
      "Epoch 117, loss: 1.984295\n",
      "Epoch 118, loss: 1.979454\n",
      "Epoch 119, loss: 1.989221\n",
      "Epoch 120, loss: 1.981080\n",
      "Epoch 121, loss: 1.979626\n",
      "Epoch 122, loss: 1.987394\n",
      "Epoch 123, loss: 1.983522\n",
      "Epoch 124, loss: 1.976683\n",
      "Epoch 125, loss: 1.979735\n",
      "Epoch 126, loss: 1.980377\n",
      "Epoch 127, loss: 1.983674\n",
      "Epoch 128, loss: 1.976860\n",
      "Epoch 129, loss: 1.976321\n",
      "Epoch 130, loss: 1.975504\n",
      "Epoch 131, loss: 1.975858\n",
      "Epoch 132, loss: 1.980903\n",
      "Epoch 133, loss: 1.974867\n",
      "Epoch 134, loss: 1.975235\n",
      "Epoch 135, loss: 1.979929\n",
      "Epoch 136, loss: 1.968255\n",
      "Epoch 137, loss: 1.969133\n",
      "Epoch 138, loss: 1.976332\n",
      "Epoch 139, loss: 1.971857\n",
      "Epoch 140, loss: 1.967442\n",
      "Epoch 141, loss: 1.965054\n",
      "Epoch 142, loss: 1.968146\n",
      "Epoch 143, loss: 1.969141\n",
      "Epoch 144, loss: 1.965057\n",
      "Epoch 145, loss: 1.966904\n",
      "Epoch 146, loss: 1.971853\n",
      "Epoch 147, loss: 1.961060\n",
      "Epoch 148, loss: 1.966448\n",
      "Epoch 149, loss: 1.963066\n",
      "Epoch 150, loss: 1.958872\n",
      "Epoch 151, loss: 1.959612\n",
      "Epoch 152, loss: 1.965714\n",
      "Epoch 153, loss: 1.959423\n",
      "Epoch 154, loss: 1.966198\n",
      "Epoch 155, loss: 1.954831\n",
      "Epoch 156, loss: 1.957570\n",
      "Epoch 157, loss: 1.965712\n",
      "Epoch 158, loss: 1.963250\n",
      "Epoch 159, loss: 1.967837\n",
      "Epoch 160, loss: 1.958091\n",
      "Epoch 161, loss: 1.958519\n",
      "Epoch 162, loss: 1.953655\n",
      "Epoch 163, loss: 1.954708\n",
      "Epoch 164, loss: 1.958235\n",
      "Epoch 165, loss: 1.952789\n",
      "Epoch 166, loss: 1.953655\n",
      "Epoch 167, loss: 1.954817\n",
      "Epoch 168, loss: 1.956241\n",
      "Epoch 169, loss: 1.955491\n",
      "Epoch 170, loss: 1.951082\n",
      "Epoch 171, loss: 1.956237\n",
      "Epoch 172, loss: 1.954532\n",
      "Epoch 173, loss: 1.953352\n",
      "Epoch 174, loss: 1.956223\n",
      "Epoch 175, loss: 1.956719\n",
      "Epoch 176, loss: 1.953080\n",
      "Epoch 177, loss: 1.947624\n",
      "Epoch 178, loss: 1.945909\n",
      "Epoch 179, loss: 1.952420\n",
      "Epoch 180, loss: 1.950881\n",
      "Epoch 181, loss: 1.954660\n",
      "Epoch 182, loss: 1.954716\n",
      "Epoch 183, loss: 1.946529\n",
      "Epoch 184, loss: 1.950953\n",
      "Epoch 185, loss: 1.949120\n",
      "Epoch 186, loss: 1.949254\n",
      "Epoch 187, loss: 1.944147\n",
      "Epoch 188, loss: 1.946594\n",
      "Epoch 189, loss: 1.945080\n",
      "Epoch 190, loss: 1.946111\n",
      "Epoch 191, loss: 1.941027\n",
      "Epoch 192, loss: 1.944476\n",
      "Epoch 193, loss: 1.942707\n",
      "Epoch 194, loss: 1.948538\n",
      "Epoch 195, loss: 1.946735\n",
      "Epoch 196, loss: 1.942903\n",
      "Epoch 197, loss: 1.934642\n",
      "Epoch 198, loss: 1.942206\n",
      "Epoch 199, loss: 1.939656\n",
      "Epoch 200, loss: 1.946667\n",
      "Epoch 201, loss: 1.939614\n",
      "Epoch 202, loss: 1.936770\n",
      "Epoch 203, loss: 1.939973\n",
      "Epoch 204, loss: 1.937527\n",
      "Epoch 205, loss: 1.939430\n",
      "Epoch 206, loss: 1.938552\n",
      "Epoch 207, loss: 1.943563\n",
      "Epoch 208, loss: 1.935535\n",
      "Epoch 209, loss: 1.932514\n",
      "Epoch 210, loss: 1.934681\n",
      "Epoch 211, loss: 1.933440\n",
      "Epoch 212, loss: 1.939635\n",
      "Epoch 213, loss: 1.931332\n",
      "Epoch 214, loss: 1.933961\n",
      "Epoch 215, loss: 1.936796\n",
      "Epoch 216, loss: 1.934758\n",
      "Epoch 217, loss: 1.932816\n",
      "Epoch 218, loss: 1.933779\n",
      "Epoch 219, loss: 1.927985\n",
      "Epoch 220, loss: 1.931879\n",
      "Epoch 221, loss: 1.935697\n",
      "Epoch 222, loss: 1.930903\n",
      "Epoch 223, loss: 1.925704\n",
      "Epoch 224, loss: 1.930575\n",
      "Epoch 225, loss: 1.927866\n",
      "Epoch 226, loss: 1.932873\n",
      "Epoch 227, loss: 1.934560\n",
      "Epoch 228, loss: 1.924354\n",
      "Epoch 229, loss: 1.924024\n",
      "Epoch 230, loss: 1.934809\n",
      "Epoch 231, loss: 1.928193\n",
      "Epoch 232, loss: 1.931740\n",
      "Epoch 233, loss: 1.924211\n",
      "Epoch 234, loss: 1.927410\n",
      "Epoch 235, loss: 1.925547\n",
      "Epoch 236, loss: 1.924784\n",
      "Epoch 237, loss: 1.924154\n",
      "Epoch 238, loss: 1.926384\n",
      "Epoch 239, loss: 1.918982\n",
      "Epoch 240, loss: 1.923615\n",
      "Epoch 241, loss: 1.927241\n",
      "Epoch 242, loss: 1.920647\n",
      "Epoch 243, loss: 1.926586\n",
      "Epoch 244, loss: 1.921622\n",
      "Epoch 245, loss: 1.923775\n",
      "Epoch 246, loss: 1.921376\n",
      "Epoch 247, loss: 1.919495\n",
      "Epoch 248, loss: 1.926461\n",
      "Epoch 249, loss: 1.918935\n",
      "Epoch 250, loss: 1.923682\n",
      "Epoch 251, loss: 1.917528\n",
      "Epoch 252, loss: 1.916268\n",
      "Epoch 253, loss: 1.919911\n",
      "Epoch 254, loss: 1.918840\n",
      "Epoch 255, loss: 1.919151\n",
      "Epoch 256, loss: 1.922179\n",
      "Epoch 257, loss: 1.916335\n",
      "Epoch 258, loss: 1.922449\n",
      "Epoch 259, loss: 1.912176\n",
      "Epoch 260, loss: 1.918856\n",
      "Epoch 261, loss: 1.912975\n",
      "Epoch 262, loss: 1.915769\n",
      "Epoch 263, loss: 1.912630\n",
      "Epoch 264, loss: 1.917421\n",
      "Epoch 265, loss: 1.914247\n",
      "Epoch 266, loss: 1.918283\n",
      "Epoch 267, loss: 1.917268\n",
      "Epoch 268, loss: 1.914517\n",
      "Epoch 269, loss: 1.920167\n",
      "Epoch 270, loss: 1.911842\n",
      "Epoch 271, loss: 1.910185\n",
      "Epoch 272, loss: 1.917456\n",
      "Epoch 273, loss: 1.913614\n",
      "Epoch 274, loss: 1.910752\n",
      "Epoch 275, loss: 1.911278\n",
      "Epoch 276, loss: 1.911226\n",
      "Epoch 277, loss: 1.917158\n",
      "Epoch 278, loss: 1.911812\n",
      "Epoch 279, loss: 1.911082\n",
      "Epoch 280, loss: 1.907074\n",
      "Epoch 281, loss: 1.910110\n",
      "Epoch 282, loss: 1.908693\n",
      "Epoch 283, loss: 1.908648\n",
      "Epoch 284, loss: 1.911186\n",
      "Epoch 285, loss: 1.904333\n",
      "Epoch 286, loss: 1.904114\n",
      "Epoch 287, loss: 1.906222\n",
      "Epoch 288, loss: 1.911894\n",
      "Epoch 289, loss: 1.902794\n",
      "Epoch 290, loss: 1.909245\n",
      "Epoch 291, loss: 1.913288\n",
      "Epoch 292, loss: 1.903227\n",
      "Epoch 293, loss: 1.902628\n",
      "Epoch 294, loss: 1.906522\n",
      "Epoch 295, loss: 1.907763\n",
      "Epoch 296, loss: 1.902765\n",
      "Epoch 297, loss: 1.902578\n",
      "Epoch 298, loss: 1.908603\n",
      "Epoch 299, loss: 1.903880\n",
      "Epoch 300, loss: 1.903296\n",
      "Epoch 301, loss: 1.902591\n",
      "Epoch 302, loss: 1.901802\n",
      "Epoch 303, loss: 1.899939\n",
      "Epoch 304, loss: 1.894355\n",
      "Epoch 305, loss: 1.904527\n",
      "Epoch 306, loss: 1.905319\n",
      "Epoch 307, loss: 1.898918\n",
      "Epoch 308, loss: 1.899348\n",
      "Epoch 309, loss: 1.895268\n",
      "Epoch 310, loss: 1.903308\n",
      "Epoch 311, loss: 1.906556\n",
      "Epoch 312, loss: 1.896423\n",
      "Epoch 313, loss: 1.905526\n",
      "Epoch 314, loss: 1.893224\n",
      "Epoch 315, loss: 1.899527\n",
      "Epoch 316, loss: 1.894126\n",
      "Epoch 317, loss: 1.901961\n",
      "Epoch 318, loss: 1.907028\n",
      "Epoch 319, loss: 1.900488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320, loss: 1.899267\n",
      "Epoch 321, loss: 1.898149\n",
      "Epoch 322, loss: 1.898164\n",
      "Epoch 323, loss: 1.896974\n",
      "Epoch 324, loss: 1.897144\n",
      "Epoch 325, loss: 1.896613\n",
      "Epoch 326, loss: 1.901186\n",
      "Epoch 327, loss: 1.896741\n",
      "Epoch 328, loss: 1.895708\n",
      "Epoch 329, loss: 1.902979\n",
      "Epoch 330, loss: 1.900421\n",
      "Epoch 331, loss: 1.896843\n",
      "Epoch 332, loss: 1.892637\n",
      "Epoch 333, loss: 1.898334\n",
      "Epoch 334, loss: 1.892402\n",
      "Epoch 335, loss: 1.898968\n",
      "Epoch 336, loss: 1.893153\n",
      "Epoch 337, loss: 1.887251\n",
      "Epoch 338, loss: 1.893578\n",
      "Epoch 339, loss: 1.894524\n",
      "Epoch 340, loss: 1.892397\n",
      "Epoch 341, loss: 1.885572\n",
      "Epoch 342, loss: 1.887040\n",
      "Epoch 343, loss: 1.893363\n",
      "Epoch 344, loss: 1.891986\n",
      "Epoch 345, loss: 1.896653\n",
      "Epoch 346, loss: 1.892610\n",
      "Epoch 347, loss: 1.891520\n",
      "Epoch 348, loss: 1.887368\n",
      "Epoch 349, loss: 1.885283\n",
      "Epoch 350, loss: 1.889748\n",
      "Epoch 351, loss: 1.889052\n",
      "Epoch 352, loss: 1.879161\n",
      "Epoch 353, loss: 1.893976\n",
      "Epoch 354, loss: 1.887834\n",
      "Epoch 355, loss: 1.883948\n",
      "Epoch 356, loss: 1.884651\n",
      "Epoch 357, loss: 1.890297\n",
      "Epoch 358, loss: 1.886136\n",
      "Epoch 359, loss: 1.889888\n",
      "Epoch 360, loss: 1.891022\n",
      "Epoch 361, loss: 1.888234\n",
      "Epoch 362, loss: 1.885336\n",
      "Epoch 363, loss: 1.885907\n",
      "Epoch 364, loss: 1.888426\n",
      "Epoch 365, loss: 1.889517\n",
      "Epoch 366, loss: 1.882511\n",
      "Epoch 367, loss: 1.883918\n",
      "Epoch 368, loss: 1.883654\n",
      "Epoch 369, loss: 1.885731\n",
      "Epoch 370, loss: 1.883498\n",
      "Epoch 371, loss: 1.880778\n",
      "Epoch 372, loss: 1.884135\n",
      "Epoch 373, loss: 1.886012\n",
      "Epoch 374, loss: 1.881075\n",
      "Epoch 375, loss: 1.887351\n",
      "Epoch 376, loss: 1.880506\n",
      "Epoch 377, loss: 1.887111\n",
      "Epoch 378, loss: 1.883133\n",
      "Epoch 379, loss: 1.879670\n",
      "Epoch 380, loss: 1.881573\n",
      "Epoch 381, loss: 1.881352\n",
      "Epoch 382, loss: 1.876121\n",
      "Epoch 383, loss: 1.884238\n",
      "Epoch 384, loss: 1.883585\n",
      "Epoch 385, loss: 1.883043\n",
      "Epoch 386, loss: 1.876208\n",
      "Epoch 387, loss: 1.873526\n",
      "Epoch 388, loss: 1.880164\n",
      "Epoch 389, loss: 1.880089\n",
      "Epoch 390, loss: 1.874794\n",
      "Epoch 391, loss: 1.878146\n",
      "Epoch 392, loss: 1.876004\n",
      "Epoch 393, loss: 1.883155\n",
      "Epoch 394, loss: 1.872848\n",
      "Epoch 395, loss: 1.875258\n",
      "Epoch 396, loss: 1.877563\n",
      "Epoch 397, loss: 1.876608\n",
      "Epoch 398, loss: 1.876135\n",
      "Epoch 399, loss: 1.875264\n",
      "Epoch 400, loss: 1.871693\n",
      "Epoch 401, loss: 1.877030\n",
      "Epoch 402, loss: 1.881175\n",
      "Epoch 403, loss: 1.871039\n",
      "Epoch 404, loss: 1.875965\n",
      "Epoch 405, loss: 1.877582\n",
      "Epoch 406, loss: 1.881914\n",
      "Epoch 407, loss: 1.873028\n",
      "Epoch 408, loss: 1.880300\n",
      "Epoch 409, loss: 1.875012\n",
      "Epoch 410, loss: 1.871080\n",
      "Epoch 411, loss: 1.875246\n",
      "Epoch 412, loss: 1.869075\n",
      "Epoch 413, loss: 1.868341\n",
      "Epoch 414, loss: 1.879945\n",
      "Epoch 415, loss: 1.875187\n",
      "Epoch 416, loss: 1.872377\n",
      "Epoch 417, loss: 1.871665\n",
      "Epoch 418, loss: 1.874018\n",
      "Epoch 419, loss: 1.867627\n",
      "Epoch 420, loss: 1.872784\n",
      "Epoch 421, loss: 1.865296\n",
      "Epoch 422, loss: 1.871929\n",
      "Epoch 423, loss: 1.867991\n",
      "Epoch 424, loss: 1.867826\n",
      "Epoch 425, loss: 1.867687\n",
      "Epoch 426, loss: 1.874959\n",
      "Epoch 427, loss: 1.870788\n",
      "Epoch 428, loss: 1.871319\n",
      "Epoch 429, loss: 1.870784\n",
      "Epoch 430, loss: 1.873046\n",
      "Epoch 431, loss: 1.866704\n",
      "Epoch 432, loss: 1.863194\n",
      "Epoch 433, loss: 1.866230\n",
      "Epoch 434, loss: 1.872330\n",
      "Epoch 435, loss: 1.865147\n",
      "Epoch 436, loss: 1.871386\n",
      "Epoch 437, loss: 1.863824\n",
      "Epoch 438, loss: 1.862297\n",
      "Epoch 439, loss: 1.860470\n",
      "Epoch 440, loss: 1.862065\n",
      "Epoch 441, loss: 1.866572\n",
      "Epoch 442, loss: 1.866143\n",
      "Epoch 443, loss: 1.865488\n",
      "Epoch 444, loss: 1.867770\n",
      "Epoch 445, loss: 1.868248\n",
      "Epoch 446, loss: 1.867667\n",
      "Epoch 447, loss: 1.866505\n",
      "Epoch 448, loss: 1.866912\n",
      "Epoch 449, loss: 1.859873\n",
      "Epoch 450, loss: 1.868621\n",
      "Epoch 451, loss: 1.861078\n",
      "Epoch 452, loss: 1.868492\n",
      "Epoch 453, loss: 1.867894\n",
      "Epoch 454, loss: 1.866742\n",
      "Epoch 455, loss: 1.858104\n",
      "Epoch 456, loss: 1.863147\n",
      "Epoch 457, loss: 1.858547\n",
      "Epoch 458, loss: 1.868488\n",
      "Epoch 459, loss: 1.865790\n",
      "Epoch 460, loss: 1.864038\n",
      "Epoch 461, loss: 1.862916\n",
      "Epoch 462, loss: 1.865240\n",
      "Epoch 463, loss: 1.865259\n",
      "Epoch 464, loss: 1.861416\n",
      "Epoch 465, loss: 1.865457\n",
      "Epoch 466, loss: 1.860777\n",
      "Epoch 467, loss: 1.854955\n",
      "Epoch 468, loss: 1.861912\n",
      "Epoch 469, loss: 1.857816\n",
      "Epoch 470, loss: 1.858386\n",
      "Epoch 471, loss: 1.865319\n",
      "Epoch 472, loss: 1.862742\n",
      "Epoch 473, loss: 1.859707\n",
      "Epoch 474, loss: 1.859201\n",
      "Epoch 475, loss: 1.862676\n",
      "Epoch 476, loss: 1.862119\n",
      "Epoch 477, loss: 1.859285\n",
      "Epoch 478, loss: 1.850366\n",
      "Epoch 479, loss: 1.857071\n",
      "Epoch 480, loss: 1.853849\n",
      "Epoch 481, loss: 1.852052\n",
      "Epoch 482, loss: 1.861405\n",
      "Epoch 483, loss: 1.849074\n",
      "Epoch 484, loss: 1.856243\n",
      "Epoch 485, loss: 1.854635\n",
      "Epoch 486, loss: 1.855112\n",
      "Epoch 487, loss: 1.854879\n",
      "Epoch 488, loss: 1.856729\n",
      "Epoch 489, loss: 1.852131\n",
      "Epoch 490, loss: 1.861410\n",
      "Epoch 491, loss: 1.850656\n",
      "Epoch 492, loss: 1.851224\n",
      "Epoch 493, loss: 1.855844\n",
      "Epoch 494, loss: 1.848390\n",
      "Epoch 495, loss: 1.853270\n",
      "Epoch 496, loss: 1.853222\n",
      "Epoch 497, loss: 1.860350\n",
      "Epoch 498, loss: 1.854515\n",
      "Epoch 499, loss: 1.846939\n",
      "Accuracy after training for 500 epochs:  learning_rate: 0.1 reg_strengths 0 -- 0.066\n",
      "Epoch 0, loss: 2.287259\n",
      "Epoch 1, loss: 2.251145\n",
      "Epoch 2, loss: 2.224765\n",
      "Epoch 3, loss: 2.207242\n",
      "Epoch 4, loss: 2.193415\n",
      "Epoch 5, loss: 2.183414\n",
      "Epoch 6, loss: 2.174124\n",
      "Epoch 7, loss: 2.168724\n",
      "Epoch 8, loss: 2.162391\n",
      "Epoch 9, loss: 2.157422\n",
      "Epoch 10, loss: 2.152543\n",
      "Epoch 11, loss: 2.149973\n",
      "Epoch 12, loss: 2.145548\n",
      "Epoch 13, loss: 2.142654\n",
      "Epoch 14, loss: 2.139607\n",
      "Epoch 15, loss: 2.137309\n",
      "Epoch 16, loss: 2.133818\n",
      "Epoch 17, loss: 2.131887\n",
      "Epoch 18, loss: 2.129429\n",
      "Epoch 19, loss: 2.127591\n",
      "Epoch 20, loss: 2.125227\n",
      "Epoch 21, loss: 2.123078\n",
      "Epoch 22, loss: 2.121605\n",
      "Epoch 23, loss: 2.120180\n",
      "Epoch 24, loss: 2.118640\n",
      "Epoch 25, loss: 2.116755\n",
      "Epoch 26, loss: 2.114730\n",
      "Epoch 27, loss: 2.113733\n",
      "Epoch 28, loss: 2.112385\n",
      "Epoch 29, loss: 2.111621\n",
      "Epoch 30, loss: 2.109522\n",
      "Epoch 31, loss: 2.108479\n",
      "Epoch 32, loss: 2.107483\n",
      "Epoch 33, loss: 2.106008\n",
      "Epoch 34, loss: 2.105248\n",
      "Epoch 35, loss: 2.103827\n",
      "Epoch 36, loss: 2.102378\n",
      "Epoch 37, loss: 2.100743\n",
      "Epoch 38, loss: 2.100568\n",
      "Epoch 39, loss: 2.098918\n",
      "Epoch 40, loss: 2.098045\n",
      "Epoch 41, loss: 2.097121\n",
      "Epoch 42, loss: 2.095946\n",
      "Epoch 43, loss: 2.096054\n",
      "Epoch 44, loss: 2.094833\n",
      "Epoch 45, loss: 2.094047\n",
      "Epoch 46, loss: 2.093369\n",
      "Epoch 47, loss: 2.092530\n",
      "Epoch 48, loss: 2.091121\n",
      "Epoch 49, loss: 2.090448\n",
      "Epoch 50, loss: 2.089925\n",
      "Epoch 51, loss: 2.088336\n",
      "Epoch 52, loss: 2.087902\n",
      "Epoch 53, loss: 2.086874\n",
      "Epoch 54, loss: 2.086805\n",
      "Epoch 55, loss: 2.085694\n",
      "Epoch 56, loss: 2.084186\n",
      "Epoch 57, loss: 2.085108\n",
      "Epoch 58, loss: 2.083516\n",
      "Epoch 59, loss: 2.082984\n",
      "Epoch 60, loss: 2.082278\n",
      "Epoch 61, loss: 2.081910\n",
      "Epoch 62, loss: 2.081088\n",
      "Epoch 63, loss: 2.080045\n",
      "Epoch 64, loss: 2.079360\n",
      "Epoch 65, loss: 2.079196\n",
      "Epoch 66, loss: 2.078351\n",
      "Epoch 67, loss: 2.077683\n",
      "Epoch 68, loss: 2.077559\n",
      "Epoch 69, loss: 2.077049\n",
      "Epoch 70, loss: 2.075209\n",
      "Epoch 71, loss: 2.075711\n",
      "Epoch 72, loss: 2.074565\n",
      "Epoch 73, loss: 2.074349\n",
      "Epoch 74, loss: 2.073371\n",
      "Epoch 75, loss: 2.073130\n",
      "Epoch 76, loss: 2.072466\n",
      "Epoch 77, loss: 2.071183\n",
      "Epoch 78, loss: 2.071532\n",
      "Epoch 79, loss: 2.070869\n",
      "Epoch 80, loss: 2.070408\n",
      "Epoch 81, loss: 2.070064\n",
      "Epoch 82, loss: 2.069656\n",
      "Epoch 83, loss: 2.068358\n",
      "Epoch 84, loss: 2.068283\n",
      "Epoch 85, loss: 2.067861\n",
      "Epoch 86, loss: 2.067709\n",
      "Epoch 87, loss: 2.066993\n",
      "Epoch 88, loss: 2.066276\n",
      "Epoch 89, loss: 2.065536\n",
      "Epoch 90, loss: 2.065410\n",
      "Epoch 91, loss: 2.064841\n",
      "Epoch 92, loss: 2.063791\n",
      "Epoch 93, loss: 2.063378\n",
      "Epoch 94, loss: 2.063228\n",
      "Epoch 95, loss: 2.062602\n",
      "Epoch 96, loss: 2.062039\n",
      "Epoch 97, loss: 2.062382\n",
      "Epoch 98, loss: 2.061950\n",
      "Epoch 99, loss: 2.061051\n",
      "Epoch 100, loss: 2.060625\n",
      "Epoch 101, loss: 2.060203\n",
      "Epoch 102, loss: 2.060144\n",
      "Epoch 103, loss: 2.059276\n",
      "Epoch 104, loss: 2.059393\n",
      "Epoch 105, loss: 2.058656\n",
      "Epoch 106, loss: 2.057699\n",
      "Epoch 107, loss: 2.057464\n",
      "Epoch 108, loss: 2.056967\n",
      "Epoch 109, loss: 2.056779\n",
      "Epoch 110, loss: 2.056506\n",
      "Epoch 111, loss: 2.055744\n",
      "Epoch 112, loss: 2.056244\n",
      "Epoch 113, loss: 2.054980\n",
      "Epoch 114, loss: 2.054440\n",
      "Epoch 115, loss: 2.053910\n",
      "Epoch 116, loss: 2.054018\n",
      "Epoch 117, loss: 2.053255\n",
      "Epoch 118, loss: 2.053007\n",
      "Epoch 119, loss: 2.053009\n",
      "Epoch 120, loss: 2.052043\n",
      "Epoch 121, loss: 2.051416\n",
      "Epoch 122, loss: 2.052075\n",
      "Epoch 123, loss: 2.051315\n",
      "Epoch 124, loss: 2.051293\n",
      "Epoch 125, loss: 2.049847\n",
      "Epoch 126, loss: 2.050511\n",
      "Epoch 127, loss: 2.049739\n",
      "Epoch 128, loss: 2.049234\n",
      "Epoch 129, loss: 2.049158\n",
      "Epoch 130, loss: 2.049420\n",
      "Epoch 131, loss: 2.048255\n",
      "Epoch 132, loss: 2.048213\n",
      "Epoch 133, loss: 2.046937\n",
      "Epoch 134, loss: 2.047680\n",
      "Epoch 135, loss: 2.047003\n",
      "Epoch 136, loss: 2.047207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137, loss: 2.046205\n",
      "Epoch 138, loss: 2.046108\n",
      "Epoch 139, loss: 2.044954\n",
      "Epoch 140, loss: 2.044868\n",
      "Epoch 141, loss: 2.045413\n",
      "Epoch 142, loss: 2.044972\n",
      "Epoch 143, loss: 2.043963\n",
      "Epoch 144, loss: 2.043349\n",
      "Epoch 145, loss: 2.043622\n",
      "Epoch 146, loss: 2.043620\n",
      "Epoch 147, loss: 2.042905\n",
      "Epoch 148, loss: 2.041811\n",
      "Epoch 149, loss: 2.042152\n",
      "Epoch 150, loss: 2.041556\n",
      "Epoch 151, loss: 2.041159\n",
      "Epoch 152, loss: 2.041378\n",
      "Epoch 153, loss: 2.039880\n",
      "Epoch 154, loss: 2.039658\n",
      "Epoch 155, loss: 2.040860\n",
      "Epoch 156, loss: 2.039596\n",
      "Epoch 157, loss: 2.039733\n",
      "Epoch 158, loss: 2.039177\n",
      "Epoch 159, loss: 2.039146\n",
      "Epoch 160, loss: 2.039072\n",
      "Epoch 161, loss: 2.038027\n",
      "Epoch 162, loss: 2.037905\n",
      "Epoch 163, loss: 2.037750\n",
      "Epoch 164, loss: 2.037485\n",
      "Epoch 165, loss: 2.037330\n",
      "Epoch 166, loss: 2.036413\n",
      "Epoch 167, loss: 2.036964\n",
      "Epoch 168, loss: 2.035481\n",
      "Epoch 169, loss: 2.035791\n",
      "Epoch 170, loss: 2.035921\n",
      "Epoch 171, loss: 2.035548\n",
      "Epoch 172, loss: 2.035549\n",
      "Epoch 173, loss: 2.034412\n",
      "Epoch 174, loss: 2.034156\n",
      "Epoch 175, loss: 2.034868\n",
      "Epoch 176, loss: 2.033536\n",
      "Epoch 177, loss: 2.032444\n",
      "Epoch 178, loss: 2.033417\n",
      "Epoch 179, loss: 2.032900\n",
      "Epoch 180, loss: 2.032537\n",
      "Epoch 181, loss: 2.032259\n",
      "Epoch 182, loss: 2.032189\n",
      "Epoch 183, loss: 2.031063\n",
      "Epoch 184, loss: 2.032015\n",
      "Epoch 185, loss: 2.031564\n",
      "Epoch 186, loss: 2.031378\n",
      "Epoch 187, loss: 2.030841\n",
      "Epoch 188, loss: 2.030767\n",
      "Epoch 189, loss: 2.030466\n",
      "Epoch 190, loss: 2.031083\n",
      "Epoch 191, loss: 2.029450\n",
      "Epoch 192, loss: 2.029733\n",
      "Epoch 193, loss: 2.028371\n",
      "Epoch 194, loss: 2.028864\n",
      "Epoch 195, loss: 2.028980\n",
      "Epoch 196, loss: 2.028399\n",
      "Epoch 197, loss: 2.027839\n",
      "Epoch 198, loss: 2.027838\n",
      "Epoch 199, loss: 2.028300\n",
      "Epoch 200, loss: 2.027074\n",
      "Epoch 201, loss: 2.026989\n",
      "Epoch 202, loss: 2.027235\n",
      "Epoch 203, loss: 2.026711\n",
      "Epoch 204, loss: 2.025956\n",
      "Epoch 205, loss: 2.026462\n",
      "Epoch 206, loss: 2.026081\n",
      "Epoch 207, loss: 2.025487\n",
      "Epoch 208, loss: 2.024933\n",
      "Epoch 209, loss: 2.025339\n",
      "Epoch 210, loss: 2.024941\n",
      "Epoch 211, loss: 2.023352\n",
      "Epoch 212, loss: 2.024109\n",
      "Epoch 213, loss: 2.024013\n",
      "Epoch 214, loss: 2.022727\n",
      "Epoch 215, loss: 2.023671\n",
      "Epoch 216, loss: 2.023639\n",
      "Epoch 217, loss: 2.023160\n",
      "Epoch 218, loss: 2.023181\n",
      "Epoch 219, loss: 2.022467\n",
      "Epoch 220, loss: 2.021817\n",
      "Epoch 221, loss: 2.021954\n",
      "Epoch 222, loss: 2.022007\n",
      "Epoch 223, loss: 2.021056\n",
      "Epoch 224, loss: 2.021511\n",
      "Epoch 225, loss: 2.021296\n",
      "Epoch 226, loss: 2.020138\n",
      "Epoch 227, loss: 2.021143\n",
      "Epoch 228, loss: 2.020070\n",
      "Epoch 229, loss: 2.020085\n",
      "Epoch 230, loss: 2.019399\n",
      "Epoch 231, loss: 2.019928\n",
      "Epoch 232, loss: 2.019663\n",
      "Epoch 233, loss: 2.019052\n",
      "Epoch 234, loss: 2.018995\n",
      "Epoch 235, loss: 2.018758\n",
      "Epoch 236, loss: 2.017459\n",
      "Epoch 237, loss: 2.018226\n",
      "Epoch 238, loss: 2.017821\n",
      "Epoch 239, loss: 2.017595\n",
      "Epoch 240, loss: 2.017753\n",
      "Epoch 241, loss: 2.017701\n",
      "Epoch 242, loss: 2.017012\n",
      "Epoch 243, loss: 2.016953\n",
      "Epoch 244, loss: 2.016712\n",
      "Epoch 245, loss: 2.016057\n",
      "Epoch 246, loss: 2.015925\n",
      "Epoch 247, loss: 2.016407\n",
      "Epoch 248, loss: 2.015783\n",
      "Epoch 249, loss: 2.015021\n",
      "Epoch 250, loss: 2.014970\n",
      "Epoch 251, loss: 2.015820\n",
      "Epoch 252, loss: 2.014602\n",
      "Epoch 253, loss: 2.014550\n",
      "Epoch 254, loss: 2.014187\n",
      "Epoch 255, loss: 2.015115\n",
      "Epoch 256, loss: 2.014348\n",
      "Epoch 257, loss: 2.014272\n",
      "Epoch 258, loss: 2.013106\n",
      "Epoch 259, loss: 2.013662\n",
      "Epoch 260, loss: 2.013698\n",
      "Epoch 261, loss: 2.013656\n",
      "Epoch 262, loss: 2.012665\n",
      "Epoch 263, loss: 2.012372\n",
      "Epoch 264, loss: 2.011993\n",
      "Epoch 265, loss: 2.012213\n",
      "Epoch 266, loss: 2.011610\n",
      "Epoch 267, loss: 2.011157\n",
      "Epoch 268, loss: 2.011163\n",
      "Epoch 269, loss: 2.011442\n",
      "Epoch 270, loss: 2.011136\n",
      "Epoch 271, loss: 2.010931\n",
      "Epoch 272, loss: 2.011777\n",
      "Epoch 273, loss: 2.010595\n",
      "Epoch 274, loss: 2.010106\n",
      "Epoch 275, loss: 2.009821\n",
      "Epoch 276, loss: 2.009904\n",
      "Epoch 277, loss: 2.009521\n",
      "Epoch 278, loss: 2.009441\n",
      "Epoch 279, loss: 2.009101\n",
      "Epoch 280, loss: 2.008885\n",
      "Epoch 281, loss: 2.008945\n",
      "Epoch 282, loss: 2.008682\n",
      "Epoch 283, loss: 2.007991\n",
      "Epoch 284, loss: 2.008650\n",
      "Epoch 285, loss: 2.007912\n",
      "Epoch 286, loss: 2.008659\n",
      "Epoch 287, loss: 2.007238\n",
      "Epoch 288, loss: 2.007139\n",
      "Epoch 289, loss: 2.007382\n",
      "Epoch 290, loss: 2.007098\n",
      "Epoch 291, loss: 2.006951\n",
      "Epoch 292, loss: 2.006531\n",
      "Epoch 293, loss: 2.006656\n",
      "Epoch 294, loss: 2.006108\n",
      "Epoch 295, loss: 2.005953\n",
      "Epoch 296, loss: 2.006027\n",
      "Epoch 297, loss: 2.005757\n",
      "Epoch 298, loss: 2.005344\n",
      "Epoch 299, loss: 2.005628\n",
      "Epoch 300, loss: 2.005190\n",
      "Epoch 301, loss: 2.004999\n",
      "Epoch 302, loss: 2.005240\n",
      "Epoch 303, loss: 2.003762\n",
      "Epoch 304, loss: 2.004301\n",
      "Epoch 305, loss: 2.005021\n",
      "Epoch 306, loss: 2.003761\n",
      "Epoch 307, loss: 2.003982\n",
      "Epoch 308, loss: 2.003358\n",
      "Epoch 309, loss: 2.003702\n",
      "Epoch 310, loss: 2.003583\n",
      "Epoch 311, loss: 2.002920\n",
      "Epoch 312, loss: 2.002118\n",
      "Epoch 313, loss: 2.001879\n",
      "Epoch 314, loss: 2.002047\n",
      "Epoch 315, loss: 2.002747\n",
      "Epoch 316, loss: 2.002439\n",
      "Epoch 317, loss: 2.001091\n",
      "Epoch 318, loss: 2.002076\n",
      "Epoch 319, loss: 2.001698\n",
      "Epoch 320, loss: 2.001321\n",
      "Epoch 321, loss: 2.000780\n",
      "Epoch 322, loss: 2.001426\n",
      "Epoch 323, loss: 2.001378\n",
      "Epoch 324, loss: 2.000460\n",
      "Epoch 325, loss: 2.000421\n",
      "Epoch 326, loss: 2.000215\n",
      "Epoch 327, loss: 2.000592\n",
      "Epoch 328, loss: 1.999409\n",
      "Epoch 329, loss: 2.000089\n",
      "Epoch 330, loss: 1.999153\n",
      "Epoch 331, loss: 1.999835\n",
      "Epoch 332, loss: 1.999820\n",
      "Epoch 333, loss: 1.998630\n",
      "Epoch 334, loss: 1.998256\n",
      "Epoch 335, loss: 1.998512\n",
      "Epoch 336, loss: 1.998988\n",
      "Epoch 337, loss: 1.997238\n",
      "Epoch 338, loss: 1.997817\n",
      "Epoch 339, loss: 1.997946\n",
      "Epoch 340, loss: 1.997414\n",
      "Epoch 341, loss: 1.997829\n",
      "Epoch 342, loss: 1.997515\n",
      "Epoch 343, loss: 1.996940\n",
      "Epoch 344, loss: 1.996867\n",
      "Epoch 345, loss: 1.997722\n",
      "Epoch 346, loss: 1.997002\n",
      "Epoch 347, loss: 1.996897\n",
      "Epoch 348, loss: 1.995902\n",
      "Epoch 349, loss: 1.996083\n",
      "Epoch 350, loss: 1.996409\n",
      "Epoch 351, loss: 1.996220\n",
      "Epoch 352, loss: 1.995503\n",
      "Epoch 353, loss: 1.995373\n",
      "Epoch 354, loss: 1.996258\n",
      "Epoch 355, loss: 1.995575\n",
      "Epoch 356, loss: 1.993818\n",
      "Epoch 357, loss: 1.995648\n",
      "Epoch 358, loss: 1.994933\n",
      "Epoch 359, loss: 1.994152\n",
      "Epoch 360, loss: 1.994423\n",
      "Epoch 361, loss: 1.994564\n",
      "Epoch 362, loss: 1.994188\n",
      "Epoch 363, loss: 1.994402\n",
      "Epoch 364, loss: 1.993467\n",
      "Epoch 365, loss: 1.993488\n",
      "Epoch 366, loss: 1.992328\n",
      "Epoch 367, loss: 1.993026\n",
      "Epoch 368, loss: 1.993417\n",
      "Epoch 369, loss: 1.993030\n",
      "Epoch 370, loss: 1.992914\n",
      "Epoch 371, loss: 1.992221\n",
      "Epoch 372, loss: 1.993031\n",
      "Epoch 373, loss: 1.992658\n",
      "Epoch 374, loss: 1.992235\n",
      "Epoch 375, loss: 1.992284\n",
      "Epoch 376, loss: 1.992222\n",
      "Epoch 377, loss: 1.991446\n",
      "Epoch 378, loss: 1.991405\n",
      "Epoch 379, loss: 1.991080\n",
      "Epoch 380, loss: 1.990910\n",
      "Epoch 381, loss: 1.991068\n",
      "Epoch 382, loss: 1.990603\n",
      "Epoch 383, loss: 1.991144\n",
      "Epoch 384, loss: 1.990196\n",
      "Epoch 385, loss: 1.990684\n",
      "Epoch 386, loss: 1.989988\n",
      "Epoch 387, loss: 1.990234\n",
      "Epoch 388, loss: 1.990131\n",
      "Epoch 389, loss: 1.989727\n",
      "Epoch 390, loss: 1.990060\n",
      "Epoch 391, loss: 1.989368\n",
      "Epoch 392, loss: 1.989585\n",
      "Epoch 393, loss: 1.989009\n",
      "Epoch 394, loss: 1.988868\n",
      "Epoch 395, loss: 1.989237\n",
      "Epoch 396, loss: 1.988725\n",
      "Epoch 397, loss: 1.988471\n",
      "Epoch 398, loss: 1.988243\n",
      "Epoch 399, loss: 1.988308\n",
      "Epoch 400, loss: 1.987605\n",
      "Epoch 401, loss: 1.987653\n",
      "Epoch 402, loss: 1.987260\n",
      "Epoch 403, loss: 1.987745\n",
      "Epoch 404, loss: 1.987706\n",
      "Epoch 405, loss: 1.987062\n",
      "Epoch 406, loss: 1.986974\n",
      "Epoch 407, loss: 1.986670\n",
      "Epoch 408, loss: 1.986301\n",
      "Epoch 409, loss: 1.986627\n",
      "Epoch 410, loss: 1.986803\n",
      "Epoch 411, loss: 1.986504\n",
      "Epoch 412, loss: 1.986502\n",
      "Epoch 413, loss: 1.986830\n",
      "Epoch 414, loss: 1.985951\n",
      "Epoch 415, loss: 1.986311\n",
      "Epoch 416, loss: 1.985607\n",
      "Epoch 417, loss: 1.985708\n",
      "Epoch 418, loss: 1.985206\n",
      "Epoch 419, loss: 1.985296\n",
      "Epoch 420, loss: 1.985273\n",
      "Epoch 421, loss: 1.984736\n",
      "Epoch 422, loss: 1.984529\n",
      "Epoch 423, loss: 1.984761\n",
      "Epoch 424, loss: 1.984541\n",
      "Epoch 425, loss: 1.983895\n",
      "Epoch 426, loss: 1.984394\n",
      "Epoch 427, loss: 1.983545\n",
      "Epoch 428, loss: 1.983869\n",
      "Epoch 429, loss: 1.983680\n",
      "Epoch 430, loss: 1.983764\n",
      "Epoch 431, loss: 1.983447\n",
      "Epoch 432, loss: 1.983432\n",
      "Epoch 433, loss: 1.982969\n",
      "Epoch 434, loss: 1.983147\n",
      "Epoch 435, loss: 1.982556\n",
      "Epoch 436, loss: 1.982326\n",
      "Epoch 437, loss: 1.982392\n",
      "Epoch 438, loss: 1.981675\n",
      "Epoch 439, loss: 1.982927\n",
      "Epoch 440, loss: 1.982698\n",
      "Epoch 441, loss: 1.982303\n",
      "Epoch 442, loss: 1.981618\n",
      "Epoch 443, loss: 1.982329\n",
      "Epoch 444, loss: 1.982117\n",
      "Epoch 445, loss: 1.981282\n",
      "Epoch 446, loss: 1.981601\n",
      "Epoch 447, loss: 1.981441\n",
      "Epoch 448, loss: 1.980978\n",
      "Epoch 449, loss: 1.980152\n",
      "Epoch 450, loss: 1.980429\n",
      "Epoch 451, loss: 1.980628\n",
      "Epoch 452, loss: 1.981091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 453, loss: 1.980172\n",
      "Epoch 454, loss: 1.979994\n",
      "Epoch 455, loss: 1.979265\n",
      "Epoch 456, loss: 1.979293\n",
      "Epoch 457, loss: 1.980333\n",
      "Epoch 458, loss: 1.979140\n",
      "Epoch 459, loss: 1.979571\n",
      "Epoch 460, loss: 1.978964\n",
      "Epoch 461, loss: 1.979454\n",
      "Epoch 462, loss: 1.978890\n",
      "Epoch 463, loss: 1.978906\n",
      "Epoch 464, loss: 1.979208\n",
      "Epoch 465, loss: 1.979150\n",
      "Epoch 466, loss: 1.978343\n",
      "Epoch 467, loss: 1.978215\n",
      "Epoch 468, loss: 1.977638\n",
      "Epoch 469, loss: 1.978320\n",
      "Epoch 470, loss: 1.978138\n",
      "Epoch 471, loss: 1.977146\n",
      "Epoch 472, loss: 1.977627\n",
      "Epoch 473, loss: 1.977057\n",
      "Epoch 474, loss: 1.977656\n",
      "Epoch 475, loss: 1.977003\n",
      "Epoch 476, loss: 1.977227\n",
      "Epoch 477, loss: 1.976574\n",
      "Epoch 478, loss: 1.976850\n",
      "Epoch 479, loss: 1.976552\n",
      "Epoch 480, loss: 1.976928\n",
      "Epoch 481, loss: 1.976363\n",
      "Epoch 482, loss: 1.976266\n",
      "Epoch 483, loss: 1.976681\n",
      "Epoch 484, loss: 1.975733\n",
      "Epoch 485, loss: 1.975156\n",
      "Epoch 486, loss: 1.975976\n",
      "Epoch 487, loss: 1.974904\n",
      "Epoch 488, loss: 1.975335\n",
      "Epoch 489, loss: 1.975405\n",
      "Epoch 490, loss: 1.974995\n",
      "Epoch 491, loss: 1.974677\n",
      "Epoch 492, loss: 1.974924\n",
      "Epoch 493, loss: 1.975356\n",
      "Epoch 494, loss: 1.975214\n",
      "Epoch 495, loss: 1.974839\n",
      "Epoch 496, loss: 1.974898\n",
      "Epoch 497, loss: 1.974110\n",
      "Epoch 498, loss: 1.974024\n",
      "Epoch 499, loss: 1.974076\n",
      "Accuracy after training for 500 epochs:  learning_rate: 0.01 reg_strengths 0 -- 0.057\n",
      "Epoch 0, loss: 2.300712\n",
      "Epoch 1, loss: 2.295287\n",
      "Epoch 2, loss: 2.290273\n",
      "Epoch 3, loss: 2.285621\n",
      "Epoch 4, loss: 2.281055\n",
      "Epoch 5, loss: 2.276727\n",
      "Epoch 6, loss: 2.272537\n",
      "Epoch 7, loss: 2.268640\n",
      "Epoch 8, loss: 2.264780\n",
      "Epoch 9, loss: 2.261130\n",
      "Epoch 10, loss: 2.257566\n",
      "Epoch 11, loss: 2.254168\n",
      "Epoch 12, loss: 2.250864\n",
      "Epoch 13, loss: 2.247754\n",
      "Epoch 14, loss: 2.244618\n",
      "Epoch 15, loss: 2.241695\n",
      "Epoch 16, loss: 2.238910\n",
      "Epoch 17, loss: 2.236129\n",
      "Epoch 18, loss: 2.233463\n",
      "Epoch 19, loss: 2.230896\n",
      "Epoch 20, loss: 2.228437\n",
      "Epoch 21, loss: 2.225951\n",
      "Epoch 22, loss: 2.223753\n",
      "Epoch 23, loss: 2.221479\n",
      "Epoch 24, loss: 2.219131\n",
      "Epoch 25, loss: 2.217147\n",
      "Epoch 26, loss: 2.215026\n",
      "Epoch 27, loss: 2.213116\n",
      "Epoch 28, loss: 2.211208\n",
      "Epoch 29, loss: 2.209388\n",
      "Epoch 30, loss: 2.207597\n",
      "Epoch 31, loss: 2.205795\n",
      "Epoch 32, loss: 2.204095\n",
      "Epoch 33, loss: 2.202487\n",
      "Epoch 34, loss: 2.200875\n",
      "Epoch 35, loss: 2.199314\n",
      "Epoch 36, loss: 2.197838\n",
      "Epoch 37, loss: 2.196446\n",
      "Epoch 38, loss: 2.194905\n",
      "Epoch 39, loss: 2.193545\n",
      "Epoch 40, loss: 2.192240\n",
      "Epoch 41, loss: 2.190898\n",
      "Epoch 42, loss: 2.189649\n",
      "Epoch 43, loss: 2.188423\n",
      "Epoch 44, loss: 2.187297\n",
      "Epoch 45, loss: 2.186040\n",
      "Epoch 46, loss: 2.184861\n",
      "Epoch 47, loss: 2.183766\n",
      "Epoch 48, loss: 2.182694\n",
      "Epoch 49, loss: 2.181715\n",
      "Epoch 50, loss: 2.180573\n",
      "Epoch 51, loss: 2.179526\n",
      "Epoch 52, loss: 2.178645\n",
      "Epoch 53, loss: 2.177496\n",
      "Epoch 54, loss: 2.176744\n",
      "Epoch 55, loss: 2.175748\n",
      "Epoch 56, loss: 2.174913\n",
      "Epoch 57, loss: 2.174058\n",
      "Epoch 58, loss: 2.173005\n",
      "Epoch 59, loss: 2.172345\n",
      "Epoch 60, loss: 2.171409\n",
      "Epoch 61, loss: 2.170699\n",
      "Epoch 62, loss: 2.169891\n",
      "Epoch 63, loss: 2.169014\n",
      "Epoch 64, loss: 2.168282\n",
      "Epoch 65, loss: 2.167611\n",
      "Epoch 66, loss: 2.166820\n",
      "Epoch 67, loss: 2.166168\n",
      "Epoch 68, loss: 2.165427\n",
      "Epoch 69, loss: 2.164794\n",
      "Epoch 70, loss: 2.164047\n",
      "Epoch 71, loss: 2.163412\n",
      "Epoch 72, loss: 2.162775\n",
      "Epoch 73, loss: 2.162165\n",
      "Epoch 74, loss: 2.161514\n",
      "Epoch 75, loss: 2.160895\n",
      "Epoch 76, loss: 2.160360\n",
      "Epoch 77, loss: 2.159586\n",
      "Epoch 78, loss: 2.159068\n",
      "Epoch 79, loss: 2.158578\n",
      "Epoch 80, loss: 2.158005\n",
      "Epoch 81, loss: 2.157357\n",
      "Epoch 82, loss: 2.156875\n",
      "Epoch 83, loss: 2.156295\n",
      "Epoch 84, loss: 2.155791\n",
      "Epoch 85, loss: 2.155236\n",
      "Epoch 86, loss: 2.154738\n",
      "Epoch 87, loss: 2.154182\n",
      "Epoch 88, loss: 2.153590\n",
      "Epoch 89, loss: 2.153125\n",
      "Epoch 90, loss: 2.152747\n",
      "Epoch 91, loss: 2.152154\n",
      "Epoch 92, loss: 2.151722\n",
      "Epoch 93, loss: 2.151283\n",
      "Epoch 94, loss: 2.150763\n",
      "Epoch 95, loss: 2.150352\n",
      "Epoch 96, loss: 2.149872\n",
      "Epoch 97, loss: 2.149412\n",
      "Epoch 98, loss: 2.148921\n",
      "Epoch 99, loss: 2.148556\n",
      "Epoch 100, loss: 2.148072\n",
      "Epoch 101, loss: 2.147679\n",
      "Epoch 102, loss: 2.147233\n",
      "Epoch 103, loss: 2.146817\n",
      "Epoch 104, loss: 2.146385\n",
      "Epoch 105, loss: 2.145930\n",
      "Epoch 106, loss: 2.145640\n",
      "Epoch 107, loss: 2.145169\n",
      "Epoch 108, loss: 2.144840\n",
      "Epoch 109, loss: 2.144429\n",
      "Epoch 110, loss: 2.143964\n",
      "Epoch 111, loss: 2.143681\n",
      "Epoch 112, loss: 2.143329\n",
      "Epoch 113, loss: 2.142917\n",
      "Epoch 114, loss: 2.142514\n",
      "Epoch 115, loss: 2.142210\n",
      "Epoch 116, loss: 2.141770\n",
      "Epoch 117, loss: 2.141405\n",
      "Epoch 118, loss: 2.141052\n",
      "Epoch 119, loss: 2.140720\n",
      "Epoch 120, loss: 2.140310\n",
      "Epoch 121, loss: 2.140069\n",
      "Epoch 122, loss: 2.139629\n",
      "Epoch 123, loss: 2.139371\n",
      "Epoch 124, loss: 2.139094\n",
      "Epoch 125, loss: 2.138651\n",
      "Epoch 126, loss: 2.138320\n",
      "Epoch 127, loss: 2.138047\n",
      "Epoch 128, loss: 2.137696\n",
      "Epoch 129, loss: 2.137334\n",
      "Epoch 130, loss: 2.137073\n",
      "Epoch 131, loss: 2.136636\n",
      "Epoch 132, loss: 2.136438\n",
      "Epoch 133, loss: 2.136090\n",
      "Epoch 134, loss: 2.135751\n",
      "Epoch 135, loss: 2.135468\n",
      "Epoch 136, loss: 2.135169\n",
      "Epoch 137, loss: 2.134923\n",
      "Epoch 138, loss: 2.134563\n",
      "Epoch 139, loss: 2.134322\n",
      "Epoch 140, loss: 2.134001\n",
      "Epoch 141, loss: 2.133660\n",
      "Epoch 142, loss: 2.133428\n",
      "Epoch 143, loss: 2.133157\n",
      "Epoch 144, loss: 2.132813\n",
      "Epoch 145, loss: 2.132531\n",
      "Epoch 146, loss: 2.132213\n",
      "Epoch 147, loss: 2.132048\n",
      "Epoch 148, loss: 2.131689\n",
      "Epoch 149, loss: 2.131378\n",
      "Epoch 150, loss: 2.131083\n",
      "Epoch 151, loss: 2.130856\n",
      "Epoch 152, loss: 2.130598\n",
      "Epoch 153, loss: 2.130383\n",
      "Epoch 154, loss: 2.130144\n",
      "Epoch 155, loss: 2.129866\n",
      "Epoch 156, loss: 2.129583\n",
      "Epoch 157, loss: 2.129298\n",
      "Epoch 158, loss: 2.128982\n",
      "Epoch 159, loss: 2.128867\n",
      "Epoch 160, loss: 2.128549\n",
      "Epoch 161, loss: 2.128239\n",
      "Epoch 162, loss: 2.128126\n",
      "Epoch 163, loss: 2.127674\n",
      "Epoch 164, loss: 2.127490\n",
      "Epoch 165, loss: 2.127291\n",
      "Epoch 166, loss: 2.127096\n",
      "Epoch 167, loss: 2.126803\n",
      "Epoch 168, loss: 2.126612\n",
      "Epoch 169, loss: 2.126325\n",
      "Epoch 170, loss: 2.126119\n",
      "Epoch 171, loss: 2.125890\n",
      "Epoch 172, loss: 2.125573\n",
      "Epoch 173, loss: 2.125391\n",
      "Epoch 174, loss: 2.125180\n",
      "Epoch 175, loss: 2.124969\n",
      "Epoch 176, loss: 2.124683\n",
      "Epoch 177, loss: 2.124405\n",
      "Epoch 178, loss: 2.124142\n",
      "Epoch 179, loss: 2.124047\n",
      "Epoch 180, loss: 2.123824\n",
      "Epoch 181, loss: 2.123533\n",
      "Epoch 182, loss: 2.123362\n",
      "Epoch 183, loss: 2.123046\n",
      "Epoch 184, loss: 2.122956\n",
      "Epoch 185, loss: 2.122702\n",
      "Epoch 186, loss: 2.122424\n",
      "Epoch 187, loss: 2.122230\n",
      "Epoch 188, loss: 2.122036\n",
      "Epoch 189, loss: 2.121882\n",
      "Epoch 190, loss: 2.121627\n",
      "Epoch 191, loss: 2.121369\n",
      "Epoch 192, loss: 2.121276\n",
      "Epoch 193, loss: 2.121019\n",
      "Epoch 194, loss: 2.120772\n",
      "Epoch 195, loss: 2.120582\n",
      "Epoch 196, loss: 2.120386\n",
      "Epoch 197, loss: 2.120144\n",
      "Epoch 198, loss: 2.119978\n",
      "Epoch 199, loss: 2.119763\n",
      "Epoch 200, loss: 2.119558\n",
      "Epoch 201, loss: 2.119289\n",
      "Epoch 202, loss: 2.119216\n",
      "Epoch 203, loss: 2.118939\n",
      "Epoch 204, loss: 2.118802\n",
      "Epoch 205, loss: 2.118694\n",
      "Epoch 206, loss: 2.118392\n",
      "Epoch 207, loss: 2.118063\n",
      "Epoch 208, loss: 2.118111\n",
      "Epoch 209, loss: 2.117834\n",
      "Epoch 210, loss: 2.117646\n",
      "Epoch 211, loss: 2.117379\n",
      "Epoch 212, loss: 2.117205\n",
      "Epoch 213, loss: 2.117097\n",
      "Epoch 214, loss: 2.116847\n",
      "Epoch 215, loss: 2.116720\n",
      "Epoch 216, loss: 2.116515\n",
      "Epoch 217, loss: 2.116371\n",
      "Epoch 218, loss: 2.116147\n",
      "Epoch 219, loss: 2.115997\n",
      "Epoch 220, loss: 2.115701\n",
      "Epoch 221, loss: 2.115584\n",
      "Epoch 222, loss: 2.115430\n",
      "Epoch 223, loss: 2.115236\n",
      "Epoch 224, loss: 2.115095\n",
      "Epoch 225, loss: 2.114943\n",
      "Epoch 226, loss: 2.114735\n",
      "Epoch 227, loss: 2.114587\n",
      "Epoch 228, loss: 2.114375\n",
      "Epoch 229, loss: 2.114145\n",
      "Epoch 230, loss: 2.113945\n",
      "Epoch 231, loss: 2.113823\n",
      "Epoch 232, loss: 2.113647\n",
      "Epoch 233, loss: 2.113500\n",
      "Epoch 234, loss: 2.113316\n",
      "Epoch 235, loss: 2.113205\n",
      "Epoch 236, loss: 2.112979\n",
      "Epoch 237, loss: 2.112845\n",
      "Epoch 238, loss: 2.112645\n",
      "Epoch 239, loss: 2.112411\n",
      "Epoch 240, loss: 2.112340\n",
      "Epoch 241, loss: 2.112134\n",
      "Epoch 242, loss: 2.112032\n",
      "Epoch 243, loss: 2.111881\n",
      "Epoch 244, loss: 2.111584\n",
      "Epoch 245, loss: 2.111478\n",
      "Epoch 246, loss: 2.111402\n",
      "Epoch 247, loss: 2.111136\n",
      "Epoch 248, loss: 2.110967\n",
      "Epoch 249, loss: 2.110877\n",
      "Epoch 250, loss: 2.110703\n",
      "Epoch 251, loss: 2.110555\n",
      "Epoch 252, loss: 2.110456\n",
      "Epoch 253, loss: 2.110275\n",
      "Epoch 254, loss: 2.110021\n",
      "Epoch 255, loss: 2.109926\n",
      "Epoch 256, loss: 2.109810\n",
      "Epoch 257, loss: 2.109638\n",
      "Epoch 258, loss: 2.109429\n",
      "Epoch 259, loss: 2.109325\n",
      "Epoch 260, loss: 2.109214\n",
      "Epoch 261, loss: 2.109004\n",
      "Epoch 262, loss: 2.108964\n",
      "Epoch 263, loss: 2.108671\n",
      "Epoch 264, loss: 2.108625\n",
      "Epoch 265, loss: 2.108399\n",
      "Epoch 266, loss: 2.108311\n",
      "Epoch 267, loss: 2.108106\n",
      "Epoch 268, loss: 2.107965\n",
      "Epoch 269, loss: 2.107907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270, loss: 2.107659\n",
      "Epoch 271, loss: 2.107572\n",
      "Epoch 272, loss: 2.107343\n",
      "Epoch 273, loss: 2.107340\n",
      "Epoch 274, loss: 2.107154\n",
      "Epoch 275, loss: 2.106971\n",
      "Epoch 276, loss: 2.106803\n",
      "Epoch 277, loss: 2.106661\n",
      "Epoch 278, loss: 2.106484\n",
      "Epoch 279, loss: 2.106436\n",
      "Epoch 280, loss: 2.106290\n",
      "Epoch 281, loss: 2.106106\n",
      "Epoch 282, loss: 2.106016\n",
      "Epoch 283, loss: 2.105820\n",
      "Epoch 284, loss: 2.105746\n",
      "Epoch 285, loss: 2.105582\n",
      "Epoch 286, loss: 2.105488\n",
      "Epoch 287, loss: 2.105236\n",
      "Epoch 288, loss: 2.105163\n",
      "Epoch 289, loss: 2.104996\n",
      "Epoch 290, loss: 2.104878\n",
      "Epoch 291, loss: 2.104739\n",
      "Epoch 292, loss: 2.104622\n",
      "Epoch 293, loss: 2.104541\n",
      "Epoch 294, loss: 2.104398\n",
      "Epoch 295, loss: 2.104189\n",
      "Epoch 296, loss: 2.104103\n",
      "Epoch 297, loss: 2.103935\n",
      "Epoch 298, loss: 2.103817\n",
      "Epoch 299, loss: 2.103680\n",
      "Epoch 300, loss: 2.103513\n",
      "Epoch 301, loss: 2.103365\n",
      "Epoch 302, loss: 2.103280\n",
      "Epoch 303, loss: 2.103178\n",
      "Epoch 304, loss: 2.103053\n",
      "Epoch 305, loss: 2.102921\n",
      "Epoch 306, loss: 2.102727\n",
      "Epoch 307, loss: 2.102679\n",
      "Epoch 308, loss: 2.102503\n",
      "Epoch 309, loss: 2.102435\n",
      "Epoch 310, loss: 2.102247\n",
      "Epoch 311, loss: 2.102123\n",
      "Epoch 312, loss: 2.102047\n",
      "Epoch 313, loss: 2.101865\n",
      "Epoch 314, loss: 2.101731\n",
      "Epoch 315, loss: 2.101623\n",
      "Epoch 316, loss: 2.101457\n",
      "Epoch 317, loss: 2.101310\n",
      "Epoch 318, loss: 2.101250\n",
      "Epoch 319, loss: 2.101066\n",
      "Epoch 320, loss: 2.101081\n",
      "Epoch 321, loss: 2.100881\n",
      "Epoch 322, loss: 2.100728\n",
      "Epoch 323, loss: 2.100691\n",
      "Epoch 324, loss: 2.100495\n",
      "Epoch 325, loss: 2.100384\n",
      "Epoch 326, loss: 2.100341\n",
      "Epoch 327, loss: 2.100131\n",
      "Epoch 328, loss: 2.100013\n",
      "Epoch 329, loss: 2.099942\n",
      "Epoch 330, loss: 2.099844\n",
      "Epoch 331, loss: 2.099679\n",
      "Epoch 332, loss: 2.099467\n",
      "Epoch 333, loss: 2.099432\n",
      "Epoch 334, loss: 2.099310\n",
      "Epoch 335, loss: 2.099280\n",
      "Epoch 336, loss: 2.099081\n",
      "Epoch 337, loss: 2.098954\n",
      "Epoch 338, loss: 2.098810\n",
      "Epoch 339, loss: 2.098725\n",
      "Epoch 340, loss: 2.098666\n",
      "Epoch 341, loss: 2.098521\n",
      "Epoch 342, loss: 2.098475\n",
      "Epoch 343, loss: 2.098258\n",
      "Epoch 344, loss: 2.098203\n",
      "Epoch 345, loss: 2.098053\n",
      "Epoch 346, loss: 2.097968\n",
      "Epoch 347, loss: 2.097797\n",
      "Epoch 348, loss: 2.097725\n",
      "Epoch 349, loss: 2.097631\n",
      "Epoch 350, loss: 2.097489\n",
      "Epoch 351, loss: 2.097356\n",
      "Epoch 352, loss: 2.097190\n",
      "Epoch 353, loss: 2.097170\n",
      "Epoch 354, loss: 2.097060\n",
      "Epoch 355, loss: 2.096945\n",
      "Epoch 356, loss: 2.096845\n",
      "Epoch 357, loss: 2.096694\n",
      "Epoch 358, loss: 2.096553\n",
      "Epoch 359, loss: 2.096531\n",
      "Epoch 360, loss: 2.096382\n",
      "Epoch 361, loss: 2.096281\n",
      "Epoch 362, loss: 2.096232\n",
      "Epoch 363, loss: 2.096072\n",
      "Epoch 364, loss: 2.095956\n",
      "Epoch 365, loss: 2.095826\n",
      "Epoch 366, loss: 2.095754\n",
      "Epoch 367, loss: 2.095592\n",
      "Epoch 368, loss: 2.095533\n",
      "Epoch 369, loss: 2.095428\n",
      "Epoch 370, loss: 2.095262\n",
      "Epoch 371, loss: 2.095217\n",
      "Epoch 372, loss: 2.095123\n",
      "Epoch 373, loss: 2.095036\n",
      "Epoch 374, loss: 2.094816\n",
      "Epoch 375, loss: 2.094791\n",
      "Epoch 376, loss: 2.094682\n",
      "Epoch 377, loss: 2.094523\n",
      "Epoch 378, loss: 2.094474\n",
      "Epoch 379, loss: 2.094386\n",
      "Epoch 380, loss: 2.094218\n",
      "Epoch 381, loss: 2.094146\n",
      "Epoch 382, loss: 2.094039\n",
      "Epoch 383, loss: 2.093933\n",
      "Epoch 384, loss: 2.093820\n",
      "Epoch 385, loss: 2.093698\n",
      "Epoch 386, loss: 2.093627\n",
      "Epoch 387, loss: 2.093526\n",
      "Epoch 388, loss: 2.093484\n",
      "Epoch 389, loss: 2.093264\n",
      "Epoch 390, loss: 2.093247\n",
      "Epoch 391, loss: 2.093172\n",
      "Epoch 392, loss: 2.092943\n",
      "Epoch 393, loss: 2.092995\n",
      "Epoch 394, loss: 2.092726\n",
      "Epoch 395, loss: 2.092754\n",
      "Epoch 396, loss: 2.092611\n",
      "Epoch 397, loss: 2.092485\n",
      "Epoch 398, loss: 2.092458\n",
      "Epoch 399, loss: 2.092332\n",
      "Epoch 400, loss: 2.092200\n",
      "Epoch 401, loss: 2.092121\n",
      "Epoch 402, loss: 2.092105\n",
      "Epoch 403, loss: 2.091924\n",
      "Epoch 404, loss: 2.091842\n",
      "Epoch 405, loss: 2.091742\n",
      "Epoch 406, loss: 2.091651\n",
      "Epoch 407, loss: 2.091572\n",
      "Epoch 408, loss: 2.091307\n",
      "Epoch 409, loss: 2.091362\n",
      "Epoch 410, loss: 2.091256\n",
      "Epoch 411, loss: 2.091064\n",
      "Epoch 412, loss: 2.091118\n",
      "Epoch 413, loss: 2.090912\n",
      "Epoch 414, loss: 2.090790\n",
      "Epoch 415, loss: 2.090737\n",
      "Epoch 416, loss: 2.090709\n",
      "Epoch 417, loss: 2.090651\n",
      "Epoch 418, loss: 2.090446\n",
      "Epoch 419, loss: 2.090405\n",
      "Epoch 420, loss: 2.090311\n",
      "Epoch 421, loss: 2.090197\n",
      "Epoch 422, loss: 2.090144\n",
      "Epoch 423, loss: 2.090029\n",
      "Epoch 424, loss: 2.089826\n",
      "Epoch 425, loss: 2.089911\n",
      "Epoch 426, loss: 2.089721\n",
      "Epoch 427, loss: 2.089676\n",
      "Epoch 428, loss: 2.089551\n",
      "Epoch 429, loss: 2.089434\n",
      "Epoch 430, loss: 2.089350\n",
      "Epoch 431, loss: 2.089295\n",
      "Epoch 432, loss: 2.089147\n",
      "Epoch 433, loss: 2.089112\n",
      "Epoch 434, loss: 2.088986\n",
      "Epoch 435, loss: 2.088898\n",
      "Epoch 436, loss: 2.088684\n",
      "Epoch 437, loss: 2.088844\n",
      "Epoch 438, loss: 2.088656\n",
      "Epoch 439, loss: 2.088544\n",
      "Epoch 440, loss: 2.088525\n",
      "Epoch 441, loss: 2.088322\n",
      "Epoch 442, loss: 2.088309\n",
      "Epoch 443, loss: 2.088227\n",
      "Epoch 444, loss: 2.088172\n",
      "Epoch 445, loss: 2.088000\n",
      "Epoch 446, loss: 2.087963\n",
      "Epoch 447, loss: 2.087766\n",
      "Epoch 448, loss: 2.087726\n",
      "Epoch 449, loss: 2.087750\n",
      "Epoch 450, loss: 2.087626\n",
      "Epoch 451, loss: 2.087502\n",
      "Epoch 452, loss: 2.087369\n",
      "Epoch 453, loss: 2.087279\n",
      "Epoch 454, loss: 2.087250\n",
      "Epoch 455, loss: 2.087185\n",
      "Epoch 456, loss: 2.087071\n",
      "Epoch 457, loss: 2.086918\n",
      "Epoch 458, loss: 2.086911\n",
      "Epoch 459, loss: 2.086792\n",
      "Epoch 460, loss: 2.086706\n",
      "Epoch 461, loss: 2.086562\n",
      "Epoch 462, loss: 2.086418\n",
      "Epoch 463, loss: 2.086440\n",
      "Epoch 464, loss: 2.086340\n",
      "Epoch 465, loss: 2.086264\n",
      "Epoch 466, loss: 2.086199\n",
      "Epoch 467, loss: 2.086130\n",
      "Epoch 468, loss: 2.085992\n",
      "Epoch 469, loss: 2.085922\n",
      "Epoch 470, loss: 2.085863\n",
      "Epoch 471, loss: 2.085748\n",
      "Epoch 472, loss: 2.085666\n",
      "Epoch 473, loss: 2.085455\n",
      "Epoch 474, loss: 2.085498\n",
      "Epoch 475, loss: 2.085450\n",
      "Epoch 476, loss: 2.085368\n",
      "Epoch 477, loss: 2.085269\n",
      "Epoch 478, loss: 2.085089\n",
      "Epoch 479, loss: 2.085081\n",
      "Epoch 480, loss: 2.085045\n",
      "Epoch 481, loss: 2.084890\n",
      "Epoch 482, loss: 2.084863\n",
      "Epoch 483, loss: 2.084686\n",
      "Epoch 484, loss: 2.084558\n",
      "Epoch 485, loss: 2.084635\n",
      "Epoch 486, loss: 2.084594\n",
      "Epoch 487, loss: 2.084372\n",
      "Epoch 488, loss: 2.084327\n",
      "Epoch 489, loss: 2.084315\n",
      "Epoch 490, loss: 2.084251\n",
      "Epoch 491, loss: 2.084124\n",
      "Epoch 492, loss: 2.084078\n",
      "Epoch 493, loss: 2.083907\n",
      "Epoch 494, loss: 2.083838\n",
      "Epoch 495, loss: 2.083786\n",
      "Epoch 496, loss: 2.083728\n",
      "Epoch 497, loss: 2.083566\n",
      "Epoch 498, loss: 2.083581\n",
      "Epoch 499, loss: 2.083457\n",
      "Accuracy after training for 500 epochs:  learning_rate: 0.001 reg_strengths 0 -- 0.053\n",
      "Epoch 0, loss: 2.302189\n",
      "Epoch 1, loss: 2.301540\n",
      "Epoch 2, loss: 2.300922\n",
      "Epoch 3, loss: 2.300323\n",
      "Epoch 4, loss: 2.299740\n",
      "Epoch 5, loss: 2.299164\n",
      "Epoch 6, loss: 2.298607\n",
      "Epoch 7, loss: 2.298055\n",
      "Epoch 8, loss: 2.297516\n",
      "Epoch 9, loss: 2.296979\n",
      "Epoch 10, loss: 2.296449\n",
      "Epoch 11, loss: 2.295921\n",
      "Epoch 12, loss: 2.295398\n",
      "Epoch 13, loss: 2.294881\n",
      "Epoch 14, loss: 2.294365\n",
      "Epoch 15, loss: 2.293859\n",
      "Epoch 16, loss: 2.293349\n",
      "Epoch 17, loss: 2.292836\n",
      "Epoch 18, loss: 2.292338\n",
      "Epoch 19, loss: 2.291841\n",
      "Epoch 20, loss: 2.291345\n",
      "Epoch 21, loss: 2.290850\n",
      "Epoch 22, loss: 2.290361\n",
      "Epoch 23, loss: 2.289876\n",
      "Epoch 24, loss: 2.289386\n",
      "Epoch 25, loss: 2.288902\n",
      "Epoch 26, loss: 2.288419\n",
      "Epoch 27, loss: 2.287941\n",
      "Epoch 28, loss: 2.287464\n",
      "Epoch 29, loss: 2.286987\n",
      "Epoch 30, loss: 2.286515\n",
      "Epoch 31, loss: 2.286049\n",
      "Epoch 32, loss: 2.285580\n",
      "Epoch 33, loss: 2.285111\n",
      "Epoch 34, loss: 2.284648\n",
      "Epoch 35, loss: 2.284186\n",
      "Epoch 36, loss: 2.283730\n",
      "Epoch 37, loss: 2.283266\n",
      "Epoch 38, loss: 2.282813\n",
      "Epoch 39, loss: 2.282364\n",
      "Epoch 40, loss: 2.281911\n",
      "Epoch 41, loss: 2.281465\n",
      "Epoch 42, loss: 2.281016\n",
      "Epoch 43, loss: 2.280568\n",
      "Epoch 44, loss: 2.280123\n",
      "Epoch 45, loss: 2.279688\n",
      "Epoch 46, loss: 2.279250\n",
      "Epoch 47, loss: 2.278810\n",
      "Epoch 48, loss: 2.278379\n",
      "Epoch 49, loss: 2.277947\n",
      "Epoch 50, loss: 2.277505\n",
      "Epoch 51, loss: 2.277084\n",
      "Epoch 52, loss: 2.276659\n",
      "Epoch 53, loss: 2.276229\n",
      "Epoch 54, loss: 2.275802\n",
      "Epoch 55, loss: 2.275385\n",
      "Epoch 56, loss: 2.274961\n",
      "Epoch 57, loss: 2.274544\n",
      "Epoch 58, loss: 2.274124\n",
      "Epoch 59, loss: 2.273715\n",
      "Epoch 60, loss: 2.273297\n",
      "Epoch 61, loss: 2.272889\n",
      "Epoch 62, loss: 2.272474\n",
      "Epoch 63, loss: 2.272072\n",
      "Epoch 64, loss: 2.271662\n",
      "Epoch 65, loss: 2.271261\n",
      "Epoch 66, loss: 2.270858\n",
      "Epoch 67, loss: 2.270452\n",
      "Epoch 68, loss: 2.270052\n",
      "Epoch 69, loss: 2.269656\n",
      "Epoch 70, loss: 2.269259\n",
      "Epoch 71, loss: 2.268864\n",
      "Epoch 72, loss: 2.268474\n",
      "Epoch 73, loss: 2.268076\n",
      "Epoch 74, loss: 2.267687\n",
      "Epoch 75, loss: 2.267299\n",
      "Epoch 76, loss: 2.266912\n",
      "Epoch 77, loss: 2.266528\n",
      "Epoch 78, loss: 2.266140\n",
      "Epoch 79, loss: 2.265762\n",
      "Epoch 80, loss: 2.265381\n",
      "Epoch 81, loss: 2.264999\n",
      "Epoch 82, loss: 2.264619\n",
      "Epoch 83, loss: 2.264245\n",
      "Epoch 84, loss: 2.263872\n",
      "Epoch 85, loss: 2.263499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, loss: 2.263126\n",
      "Epoch 87, loss: 2.262756\n",
      "Epoch 88, loss: 2.262385\n",
      "Epoch 89, loss: 2.262019\n",
      "Epoch 90, loss: 2.261654\n",
      "Epoch 91, loss: 2.261285\n",
      "Epoch 92, loss: 2.260924\n",
      "Epoch 93, loss: 2.260559\n",
      "Epoch 94, loss: 2.260197\n",
      "Epoch 95, loss: 2.259839\n",
      "Epoch 96, loss: 2.259487\n",
      "Epoch 97, loss: 2.259124\n",
      "Epoch 98, loss: 2.258770\n",
      "Epoch 99, loss: 2.258418\n",
      "Epoch 100, loss: 2.258066\n",
      "Epoch 101, loss: 2.257717\n",
      "Epoch 102, loss: 2.257361\n",
      "Epoch 103, loss: 2.257013\n",
      "Epoch 104, loss: 2.256664\n",
      "Epoch 105, loss: 2.256322\n",
      "Epoch 106, loss: 2.255975\n",
      "Epoch 107, loss: 2.255635\n",
      "Epoch 108, loss: 2.255289\n",
      "Epoch 109, loss: 2.254950\n",
      "Epoch 110, loss: 2.254610\n",
      "Epoch 111, loss: 2.254273\n",
      "Epoch 112, loss: 2.253933\n",
      "Epoch 113, loss: 2.253597\n",
      "Epoch 114, loss: 2.253261\n",
      "Epoch 115, loss: 2.252934\n",
      "Epoch 116, loss: 2.252603\n",
      "Epoch 117, loss: 2.252267\n",
      "Epoch 118, loss: 2.251932\n",
      "Epoch 119, loss: 2.251607\n",
      "Epoch 120, loss: 2.251278\n",
      "Epoch 121, loss: 2.250955\n",
      "Epoch 122, loss: 2.250628\n",
      "Epoch 123, loss: 2.250301\n",
      "Epoch 124, loss: 2.249983\n",
      "Epoch 125, loss: 2.249665\n",
      "Epoch 126, loss: 2.249343\n",
      "Epoch 127, loss: 2.249020\n",
      "Epoch 128, loss: 2.248706\n",
      "Epoch 129, loss: 2.248388\n",
      "Epoch 130, loss: 2.248072\n",
      "Epoch 131, loss: 2.247759\n",
      "Epoch 132, loss: 2.247440\n",
      "Epoch 133, loss: 2.247127\n",
      "Epoch 134, loss: 2.246815\n",
      "Epoch 135, loss: 2.246506\n",
      "Epoch 136, loss: 2.246200\n",
      "Epoch 137, loss: 2.245888\n",
      "Epoch 138, loss: 2.245581\n",
      "Epoch 139, loss: 2.245279\n",
      "Epoch 140, loss: 2.244977\n",
      "Epoch 141, loss: 2.244669\n",
      "Epoch 142, loss: 2.244363\n",
      "Epoch 143, loss: 2.244069\n",
      "Epoch 144, loss: 2.243765\n",
      "Epoch 145, loss: 2.243463\n",
      "Epoch 146, loss: 2.243168\n",
      "Epoch 147, loss: 2.242869\n",
      "Epoch 148, loss: 2.242571\n",
      "Epoch 149, loss: 2.242276\n",
      "Epoch 150, loss: 2.241985\n",
      "Epoch 151, loss: 2.241690\n",
      "Epoch 152, loss: 2.241397\n",
      "Epoch 153, loss: 2.241111\n",
      "Epoch 154, loss: 2.240815\n",
      "Epoch 155, loss: 2.240527\n",
      "Epoch 156, loss: 2.240239\n",
      "Epoch 157, loss: 2.239955\n",
      "Epoch 158, loss: 2.239668\n",
      "Epoch 159, loss: 2.239383\n",
      "Epoch 160, loss: 2.239098\n",
      "Epoch 161, loss: 2.238819\n",
      "Epoch 162, loss: 2.238533\n",
      "Epoch 163, loss: 2.238252\n",
      "Epoch 164, loss: 2.237974\n",
      "Epoch 165, loss: 2.237688\n",
      "Epoch 166, loss: 2.237412\n",
      "Epoch 167, loss: 2.237136\n",
      "Epoch 168, loss: 2.236859\n",
      "Epoch 169, loss: 2.236584\n",
      "Epoch 170, loss: 2.236309\n",
      "Epoch 171, loss: 2.236039\n",
      "Epoch 172, loss: 2.235768\n",
      "Epoch 173, loss: 2.235492\n",
      "Epoch 174, loss: 2.235221\n",
      "Epoch 175, loss: 2.234956\n",
      "Epoch 176, loss: 2.234688\n",
      "Epoch 177, loss: 2.234415\n",
      "Epoch 178, loss: 2.234148\n",
      "Epoch 179, loss: 2.233880\n",
      "Epoch 180, loss: 2.233616\n",
      "Epoch 181, loss: 2.233348\n",
      "Epoch 182, loss: 2.233087\n",
      "Epoch 183, loss: 2.232825\n",
      "Epoch 184, loss: 2.232560\n",
      "Epoch 185, loss: 2.232306\n",
      "Epoch 186, loss: 2.232039\n",
      "Epoch 187, loss: 2.231785\n",
      "Epoch 188, loss: 2.231526\n",
      "Epoch 189, loss: 2.231269\n",
      "Epoch 190, loss: 2.231015\n",
      "Epoch 191, loss: 2.230752\n",
      "Epoch 192, loss: 2.230498\n",
      "Epoch 193, loss: 2.230248\n",
      "Epoch 194, loss: 2.229997\n",
      "Epoch 195, loss: 2.229747\n",
      "Epoch 196, loss: 2.229491\n",
      "Epoch 197, loss: 2.229247\n",
      "Epoch 198, loss: 2.228993\n",
      "Epoch 199, loss: 2.228743\n",
      "Epoch 200, loss: 2.228494\n",
      "Epoch 201, loss: 2.228250\n",
      "Epoch 202, loss: 2.228000\n",
      "Epoch 203, loss: 2.227756\n",
      "Epoch 204, loss: 2.227513\n",
      "Epoch 205, loss: 2.227266\n",
      "Epoch 206, loss: 2.227026\n",
      "Epoch 207, loss: 2.226786\n",
      "Epoch 208, loss: 2.226537\n",
      "Epoch 209, loss: 2.226298\n",
      "Epoch 210, loss: 2.226063\n",
      "Epoch 211, loss: 2.225818\n",
      "Epoch 212, loss: 2.225584\n",
      "Epoch 213, loss: 2.225346\n",
      "Epoch 214, loss: 2.225114\n",
      "Epoch 215, loss: 2.224869\n",
      "Epoch 216, loss: 2.224636\n",
      "Epoch 217, loss: 2.224407\n",
      "Epoch 218, loss: 2.224169\n",
      "Epoch 219, loss: 2.223936\n",
      "Epoch 220, loss: 2.223702\n",
      "Epoch 221, loss: 2.223474\n",
      "Epoch 222, loss: 2.223242\n",
      "Epoch 223, loss: 2.223018\n",
      "Epoch 224, loss: 2.222781\n",
      "Epoch 225, loss: 2.222558\n",
      "Epoch 226, loss: 2.222330\n",
      "Epoch 227, loss: 2.222101\n",
      "Epoch 228, loss: 2.221879\n",
      "Epoch 229, loss: 2.221650\n",
      "Epoch 230, loss: 2.221426\n",
      "Epoch 231, loss: 2.221203\n",
      "Epoch 232, loss: 2.220985\n",
      "Epoch 233, loss: 2.220762\n",
      "Epoch 234, loss: 2.220540\n",
      "Epoch 235, loss: 2.220314\n",
      "Epoch 236, loss: 2.220095\n",
      "Epoch 237, loss: 2.219872\n",
      "Epoch 238, loss: 2.219658\n",
      "Epoch 239, loss: 2.219440\n",
      "Epoch 240, loss: 2.219221\n",
      "Epoch 241, loss: 2.219001\n",
      "Epoch 242, loss: 2.218789\n",
      "Epoch 243, loss: 2.218572\n",
      "Epoch 244, loss: 2.218359\n",
      "Epoch 245, loss: 2.218144\n",
      "Epoch 246, loss: 2.217932\n",
      "Epoch 247, loss: 2.217720\n",
      "Epoch 248, loss: 2.217510\n",
      "Epoch 249, loss: 2.217296\n",
      "Epoch 250, loss: 2.217085\n",
      "Epoch 251, loss: 2.216874\n",
      "Epoch 252, loss: 2.216669\n",
      "Epoch 253, loss: 2.216459\n",
      "Epoch 254, loss: 2.216252\n",
      "Epoch 255, loss: 2.216044\n",
      "Epoch 256, loss: 2.215838\n",
      "Epoch 257, loss: 2.215628\n",
      "Epoch 258, loss: 2.215424\n",
      "Epoch 259, loss: 2.215227\n",
      "Epoch 260, loss: 2.215020\n",
      "Epoch 261, loss: 2.214813\n",
      "Epoch 262, loss: 2.214609\n",
      "Epoch 263, loss: 2.214412\n",
      "Epoch 264, loss: 2.214211\n",
      "Epoch 265, loss: 2.214010\n",
      "Epoch 266, loss: 2.213812\n",
      "Epoch 267, loss: 2.213606\n",
      "Epoch 268, loss: 2.213411\n",
      "Epoch 269, loss: 2.213215\n",
      "Epoch 270, loss: 2.213019\n",
      "Epoch 271, loss: 2.212824\n",
      "Epoch 272, loss: 2.212624\n",
      "Epoch 273, loss: 2.212429\n",
      "Epoch 274, loss: 2.212233\n",
      "Epoch 275, loss: 2.212042\n",
      "Epoch 276, loss: 2.211842\n",
      "Epoch 277, loss: 2.211652\n",
      "Epoch 278, loss: 2.211455\n",
      "Epoch 279, loss: 2.211273\n",
      "Epoch 280, loss: 2.211072\n",
      "Epoch 281, loss: 2.210887\n",
      "Epoch 282, loss: 2.210699\n",
      "Epoch 283, loss: 2.210506\n",
      "Epoch 284, loss: 2.210317\n",
      "Epoch 285, loss: 2.210126\n",
      "Epoch 286, loss: 2.209937\n",
      "Epoch 287, loss: 2.209751\n",
      "Epoch 288, loss: 2.209569\n",
      "Epoch 289, loss: 2.209379\n",
      "Epoch 290, loss: 2.209196\n",
      "Epoch 291, loss: 2.209009\n",
      "Epoch 292, loss: 2.208824\n",
      "Epoch 293, loss: 2.208640\n",
      "Epoch 294, loss: 2.208458\n",
      "Epoch 295, loss: 2.208275\n",
      "Epoch 296, loss: 2.208097\n",
      "Epoch 297, loss: 2.207909\n",
      "Epoch 298, loss: 2.207734\n",
      "Epoch 299, loss: 2.207554\n",
      "Epoch 300, loss: 2.207371\n",
      "Epoch 301, loss: 2.207194\n",
      "Epoch 302, loss: 2.207021\n",
      "Epoch 303, loss: 2.206838\n",
      "Epoch 304, loss: 2.206663\n",
      "Epoch 305, loss: 2.206484\n",
      "Epoch 306, loss: 2.206305\n",
      "Epoch 307, loss: 2.206132\n",
      "Epoch 308, loss: 2.205956\n",
      "Epoch 309, loss: 2.205779\n",
      "Epoch 310, loss: 2.205604\n",
      "Epoch 311, loss: 2.205434\n",
      "Epoch 312, loss: 2.205258\n",
      "Epoch 313, loss: 2.205087\n",
      "Epoch 314, loss: 2.204909\n",
      "Epoch 315, loss: 2.204738\n",
      "Epoch 316, loss: 2.204568\n",
      "Epoch 317, loss: 2.204398\n",
      "Epoch 318, loss: 2.204227\n",
      "Epoch 319, loss: 2.204066\n",
      "Epoch 320, loss: 2.203891\n",
      "Epoch 321, loss: 2.203725\n",
      "Epoch 322, loss: 2.203554\n",
      "Epoch 323, loss: 2.203389\n",
      "Epoch 324, loss: 2.203218\n",
      "Epoch 325, loss: 2.203054\n",
      "Epoch 326, loss: 2.202885\n",
      "Epoch 327, loss: 2.202726\n",
      "Epoch 328, loss: 2.202557\n",
      "Epoch 329, loss: 2.202393\n",
      "Epoch 330, loss: 2.202229\n",
      "Epoch 331, loss: 2.202069\n",
      "Epoch 332, loss: 2.201904\n",
      "Epoch 333, loss: 2.201738\n",
      "Epoch 334, loss: 2.201579\n",
      "Epoch 335, loss: 2.201417\n",
      "Epoch 336, loss: 2.201253\n",
      "Epoch 337, loss: 2.201092\n",
      "Epoch 338, loss: 2.200935\n",
      "Epoch 339, loss: 2.200775\n",
      "Epoch 340, loss: 2.200612\n",
      "Epoch 341, loss: 2.200457\n",
      "Epoch 342, loss: 2.200301\n",
      "Epoch 343, loss: 2.200140\n",
      "Epoch 344, loss: 2.199982\n",
      "Epoch 345, loss: 2.199826\n",
      "Epoch 346, loss: 2.199666\n",
      "Epoch 347, loss: 2.199511\n",
      "Epoch 348, loss: 2.199358\n",
      "Epoch 349, loss: 2.199206\n",
      "Epoch 350, loss: 2.199047\n",
      "Epoch 351, loss: 2.198893\n",
      "Epoch 352, loss: 2.198744\n",
      "Epoch 353, loss: 2.198584\n",
      "Epoch 354, loss: 2.198436\n",
      "Epoch 355, loss: 2.198284\n",
      "Epoch 356, loss: 2.198129\n",
      "Epoch 357, loss: 2.197979\n",
      "Epoch 358, loss: 2.197828\n",
      "Epoch 359, loss: 2.197678\n",
      "Epoch 360, loss: 2.197525\n",
      "Epoch 361, loss: 2.197375\n",
      "Epoch 362, loss: 2.197226\n",
      "Epoch 363, loss: 2.197078\n",
      "Epoch 364, loss: 2.196930\n",
      "Epoch 365, loss: 2.196784\n",
      "Epoch 366, loss: 2.196634\n",
      "Epoch 367, loss: 2.196489\n",
      "Epoch 368, loss: 2.196338\n",
      "Epoch 369, loss: 2.196194\n",
      "Epoch 370, loss: 2.196049\n",
      "Epoch 371, loss: 2.195904\n",
      "Epoch 372, loss: 2.195757\n",
      "Epoch 373, loss: 2.195613\n",
      "Epoch 374, loss: 2.195469\n",
      "Epoch 375, loss: 2.195325\n",
      "Epoch 376, loss: 2.195185\n",
      "Epoch 377, loss: 2.195040\n",
      "Epoch 378, loss: 2.194894\n",
      "Epoch 379, loss: 2.194756\n",
      "Epoch 380, loss: 2.194607\n",
      "Epoch 381, loss: 2.194472\n",
      "Epoch 382, loss: 2.194331\n",
      "Epoch 383, loss: 2.194192\n",
      "Epoch 384, loss: 2.194052\n",
      "Epoch 385, loss: 2.193911\n",
      "Epoch 386, loss: 2.193770\n",
      "Epoch 387, loss: 2.193631\n",
      "Epoch 388, loss: 2.193492\n",
      "Epoch 389, loss: 2.193355\n",
      "Epoch 390, loss: 2.193217\n",
      "Epoch 391, loss: 2.193080\n",
      "Epoch 392, loss: 2.192941\n",
      "Epoch 393, loss: 2.192805\n",
      "Epoch 394, loss: 2.192673\n",
      "Epoch 395, loss: 2.192538\n",
      "Epoch 396, loss: 2.192400\n",
      "Epoch 397, loss: 2.192264\n",
      "Epoch 398, loss: 2.192125\n",
      "Epoch 399, loss: 2.191994\n",
      "Epoch 400, loss: 2.191859\n",
      "Epoch 401, loss: 2.191729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 402, loss: 2.191596\n",
      "Epoch 403, loss: 2.191457\n",
      "Epoch 404, loss: 2.191323\n",
      "Epoch 405, loss: 2.191194\n",
      "Epoch 406, loss: 2.191061\n",
      "Epoch 407, loss: 2.190929\n",
      "Epoch 408, loss: 2.190795\n",
      "Epoch 409, loss: 2.190671\n",
      "Epoch 410, loss: 2.190537\n",
      "Epoch 411, loss: 2.190406\n",
      "Epoch 412, loss: 2.190281\n",
      "Epoch 413, loss: 2.190151\n",
      "Epoch 414, loss: 2.190024\n",
      "Epoch 415, loss: 2.189897\n",
      "Epoch 416, loss: 2.189768\n",
      "Epoch 417, loss: 2.189636\n",
      "Epoch 418, loss: 2.189507\n",
      "Epoch 419, loss: 2.189383\n",
      "Epoch 420, loss: 2.189257\n",
      "Epoch 421, loss: 2.189131\n",
      "Epoch 422, loss: 2.189006\n",
      "Epoch 423, loss: 2.188878\n",
      "Epoch 424, loss: 2.188749\n",
      "Epoch 425, loss: 2.188628\n",
      "Epoch 426, loss: 2.188507\n",
      "Epoch 427, loss: 2.188377\n",
      "Epoch 428, loss: 2.188254\n",
      "Epoch 429, loss: 2.188128\n",
      "Epoch 430, loss: 2.188008\n",
      "Epoch 431, loss: 2.187886\n",
      "Epoch 432, loss: 2.187760\n",
      "Epoch 433, loss: 2.187640\n",
      "Epoch 434, loss: 2.187518\n",
      "Epoch 435, loss: 2.187395\n",
      "Epoch 436, loss: 2.187270\n",
      "Epoch 437, loss: 2.187149\n",
      "Epoch 438, loss: 2.187028\n",
      "Epoch 439, loss: 2.186908\n",
      "Epoch 440, loss: 2.186794\n",
      "Epoch 441, loss: 2.186664\n",
      "Epoch 442, loss: 2.186550\n",
      "Epoch 443, loss: 2.186429\n",
      "Epoch 444, loss: 2.186311\n",
      "Epoch 445, loss: 2.186192\n",
      "Epoch 446, loss: 2.186076\n",
      "Epoch 447, loss: 2.185956\n",
      "Epoch 448, loss: 2.185837\n",
      "Epoch 449, loss: 2.185721\n",
      "Epoch 450, loss: 2.185604\n",
      "Epoch 451, loss: 2.185491\n",
      "Epoch 452, loss: 2.185366\n",
      "Epoch 453, loss: 2.185253\n",
      "Epoch 454, loss: 2.185137\n",
      "Epoch 455, loss: 2.185023\n",
      "Epoch 456, loss: 2.184908\n",
      "Epoch 457, loss: 2.184791\n",
      "Epoch 458, loss: 2.184681\n",
      "Epoch 459, loss: 2.184564\n",
      "Epoch 460, loss: 2.184452\n",
      "Epoch 461, loss: 2.184338\n",
      "Epoch 462, loss: 2.184220\n",
      "Epoch 463, loss: 2.184113\n",
      "Epoch 464, loss: 2.183996\n",
      "Epoch 465, loss: 2.183877\n",
      "Epoch 466, loss: 2.183769\n",
      "Epoch 467, loss: 2.183657\n",
      "Epoch 468, loss: 2.183548\n",
      "Epoch 469, loss: 2.183436\n",
      "Epoch 470, loss: 2.183325\n",
      "Epoch 471, loss: 2.183213\n",
      "Epoch 472, loss: 2.183103\n",
      "Epoch 473, loss: 2.182993\n",
      "Epoch 474, loss: 2.182887\n",
      "Epoch 475, loss: 2.182776\n",
      "Epoch 476, loss: 2.182667\n",
      "Epoch 477, loss: 2.182554\n",
      "Epoch 478, loss: 2.182448\n",
      "Epoch 479, loss: 2.182336\n",
      "Epoch 480, loss: 2.182230\n",
      "Epoch 481, loss: 2.182115\n",
      "Epoch 482, loss: 2.182016\n",
      "Epoch 483, loss: 2.181904\n",
      "Epoch 484, loss: 2.181795\n",
      "Epoch 485, loss: 2.181693\n",
      "Epoch 486, loss: 2.181582\n",
      "Epoch 487, loss: 2.181476\n",
      "Epoch 488, loss: 2.181369\n",
      "Epoch 489, loss: 2.181267\n",
      "Epoch 490, loss: 2.181158\n",
      "Epoch 491, loss: 2.181056\n",
      "Epoch 492, loss: 2.180948\n",
      "Epoch 493, loss: 2.180841\n",
      "Epoch 494, loss: 2.180736\n",
      "Epoch 495, loss: 2.180637\n",
      "Epoch 496, loss: 2.180529\n",
      "Epoch 497, loss: 2.180427\n",
      "Epoch 498, loss: 2.180322\n",
      "Epoch 499, loss: 2.180221\n",
      "Accuracy after training for 500 epochs:  learning_rate: 0.0001 reg_strengths 0 -- 0.048\n",
      "Epoch 0, loss: 2.302533\n",
      "Epoch 1, loss: 2.302465\n",
      "Epoch 2, loss: 2.302397\n",
      "Epoch 3, loss: 2.302330\n",
      "Epoch 4, loss: 2.302263\n",
      "Epoch 5, loss: 2.302197\n",
      "Epoch 6, loss: 2.302131\n",
      "Epoch 7, loss: 2.302065\n",
      "Epoch 8, loss: 2.302000\n",
      "Epoch 9, loss: 2.301934\n",
      "Epoch 10, loss: 2.301870\n",
      "Epoch 11, loss: 2.301804\n",
      "Epoch 12, loss: 2.301741\n",
      "Epoch 13, loss: 2.301676\n",
      "Epoch 14, loss: 2.301612\n",
      "Epoch 15, loss: 2.301549\n",
      "Epoch 16, loss: 2.301485\n",
      "Epoch 17, loss: 2.301423\n",
      "Epoch 18, loss: 2.301359\n",
      "Epoch 19, loss: 2.301297\n",
      "Epoch 20, loss: 2.301235\n",
      "Epoch 21, loss: 2.301172\n",
      "Epoch 22, loss: 2.301110\n",
      "Epoch 23, loss: 2.301049\n",
      "Epoch 24, loss: 2.300987\n",
      "Epoch 25, loss: 2.300926\n",
      "Epoch 26, loss: 2.300865\n",
      "Epoch 27, loss: 2.300804\n",
      "Epoch 28, loss: 2.300743\n",
      "Epoch 29, loss: 2.300683\n",
      "Epoch 30, loss: 2.300623\n",
      "Epoch 31, loss: 2.300562\n",
      "Epoch 32, loss: 2.300503\n",
      "Epoch 33, loss: 2.300443\n",
      "Epoch 34, loss: 2.300383\n",
      "Epoch 35, loss: 2.300324\n",
      "Epoch 36, loss: 2.300264\n",
      "Epoch 37, loss: 2.300205\n",
      "Epoch 38, loss: 2.300146\n",
      "Epoch 39, loss: 2.300087\n",
      "Epoch 40, loss: 2.300029\n",
      "Epoch 41, loss: 2.299970\n",
      "Epoch 42, loss: 2.299912\n",
      "Epoch 43, loss: 2.299853\n",
      "Epoch 44, loss: 2.299796\n",
      "Epoch 45, loss: 2.299738\n",
      "Epoch 46, loss: 2.299680\n",
      "Epoch 47, loss: 2.299622\n",
      "Epoch 48, loss: 2.299565\n",
      "Epoch 49, loss: 2.299507\n",
      "Epoch 50, loss: 2.299450\n",
      "Epoch 51, loss: 2.299392\n",
      "Epoch 52, loss: 2.299335\n",
      "Epoch 53, loss: 2.299279\n",
      "Epoch 54, loss: 2.299222\n",
      "Epoch 55, loss: 2.299165\n",
      "Epoch 56, loss: 2.299109\n",
      "Epoch 57, loss: 2.299052\n",
      "Epoch 58, loss: 2.298996\n",
      "Epoch 59, loss: 2.298940\n",
      "Epoch 60, loss: 2.298883\n",
      "Epoch 61, loss: 2.298827\n",
      "Epoch 62, loss: 2.298771\n",
      "Epoch 63, loss: 2.298715\n",
      "Epoch 64, loss: 2.298659\n",
      "Epoch 65, loss: 2.298604\n",
      "Epoch 66, loss: 2.298548\n",
      "Epoch 67, loss: 2.298493\n",
      "Epoch 68, loss: 2.298437\n",
      "Epoch 69, loss: 2.298382\n",
      "Epoch 70, loss: 2.298327\n",
      "Epoch 71, loss: 2.298271\n",
      "Epoch 72, loss: 2.298217\n",
      "Epoch 73, loss: 2.298161\n",
      "Epoch 74, loss: 2.298106\n",
      "Epoch 75, loss: 2.298051\n",
      "Epoch 76, loss: 2.297997\n",
      "Epoch 77, loss: 2.297942\n",
      "Epoch 78, loss: 2.297888\n",
      "Epoch 79, loss: 2.297833\n",
      "Epoch 80, loss: 2.297778\n",
      "Epoch 81, loss: 2.297724\n",
      "Epoch 82, loss: 2.297670\n",
      "Epoch 83, loss: 2.297615\n",
      "Epoch 84, loss: 2.297561\n",
      "Epoch 85, loss: 2.297507\n",
      "Epoch 86, loss: 2.297452\n",
      "Epoch 87, loss: 2.297399\n",
      "Epoch 88, loss: 2.297345\n",
      "Epoch 89, loss: 2.297291\n",
      "Epoch 90, loss: 2.297237\n",
      "Epoch 91, loss: 2.297183\n",
      "Epoch 92, loss: 2.297129\n",
      "Epoch 93, loss: 2.297076\n",
      "Epoch 94, loss: 2.297022\n",
      "Epoch 95, loss: 2.296969\n",
      "Epoch 96, loss: 2.296915\n",
      "Epoch 97, loss: 2.296862\n",
      "Epoch 98, loss: 2.296808\n",
      "Epoch 99, loss: 2.296756\n",
      "Epoch 100, loss: 2.296702\n",
      "Epoch 101, loss: 2.296649\n",
      "Epoch 102, loss: 2.296595\n",
      "Epoch 103, loss: 2.296542\n",
      "Epoch 104, loss: 2.296489\n",
      "Epoch 105, loss: 2.296436\n",
      "Epoch 106, loss: 2.296383\n",
      "Epoch 107, loss: 2.296331\n",
      "Epoch 108, loss: 2.296278\n",
      "Epoch 109, loss: 2.296225\n",
      "Epoch 110, loss: 2.296172\n",
      "Epoch 111, loss: 2.296120\n",
      "Epoch 112, loss: 2.296067\n",
      "Epoch 113, loss: 2.296014\n",
      "Epoch 114, loss: 2.295962\n",
      "Epoch 115, loss: 2.295909\n",
      "Epoch 116, loss: 2.295856\n",
      "Epoch 117, loss: 2.295804\n",
      "Epoch 118, loss: 2.295752\n",
      "Epoch 119, loss: 2.295700\n",
      "Epoch 120, loss: 2.295647\n",
      "Epoch 121, loss: 2.295595\n",
      "Epoch 122, loss: 2.295543\n",
      "Epoch 123, loss: 2.295491\n",
      "Epoch 124, loss: 2.295439\n",
      "Epoch 125, loss: 2.295386\n",
      "Epoch 126, loss: 2.295334\n",
      "Epoch 127, loss: 2.295282\n",
      "Epoch 128, loss: 2.295230\n",
      "Epoch 129, loss: 2.295178\n",
      "Epoch 130, loss: 2.295126\n",
      "Epoch 131, loss: 2.295075\n",
      "Epoch 132, loss: 2.295022\n",
      "Epoch 133, loss: 2.294971\n",
      "Epoch 134, loss: 2.294919\n",
      "Epoch 135, loss: 2.294867\n",
      "Epoch 136, loss: 2.294816\n",
      "Epoch 137, loss: 2.294764\n",
      "Epoch 138, loss: 2.294713\n",
      "Epoch 139, loss: 2.294661\n",
      "Epoch 140, loss: 2.294609\n",
      "Epoch 141, loss: 2.294558\n",
      "Epoch 142, loss: 2.294507\n",
      "Epoch 143, loss: 2.294455\n",
      "Epoch 144, loss: 2.294404\n",
      "Epoch 145, loss: 2.294352\n",
      "Epoch 146, loss: 2.294301\n",
      "Epoch 147, loss: 2.294250\n",
      "Epoch 148, loss: 2.294199\n",
      "Epoch 149, loss: 2.294147\n",
      "Epoch 150, loss: 2.294096\n",
      "Epoch 151, loss: 2.294045\n",
      "Epoch 152, loss: 2.293994\n",
      "Epoch 153, loss: 2.293943\n",
      "Epoch 154, loss: 2.293892\n",
      "Epoch 155, loss: 2.293840\n",
      "Epoch 156, loss: 2.293790\n",
      "Epoch 157, loss: 2.293739\n",
      "Epoch 158, loss: 2.293688\n",
      "Epoch 159, loss: 2.293637\n",
      "Epoch 160, loss: 2.293586\n",
      "Epoch 161, loss: 2.293535\n",
      "Epoch 162, loss: 2.293485\n",
      "Epoch 163, loss: 2.293434\n",
      "Epoch 164, loss: 2.293383\n",
      "Epoch 165, loss: 2.293332\n",
      "Epoch 166, loss: 2.293282\n",
      "Epoch 167, loss: 2.293231\n",
      "Epoch 168, loss: 2.293180\n",
      "Epoch 169, loss: 2.293129\n",
      "Epoch 170, loss: 2.293079\n",
      "Epoch 171, loss: 2.293028\n",
      "Epoch 172, loss: 2.292978\n",
      "Epoch 173, loss: 2.292927\n",
      "Epoch 174, loss: 2.292877\n",
      "Epoch 175, loss: 2.292827\n",
      "Epoch 176, loss: 2.292776\n",
      "Epoch 177, loss: 2.292726\n",
      "Epoch 178, loss: 2.292676\n",
      "Epoch 179, loss: 2.292625\n",
      "Epoch 180, loss: 2.292575\n",
      "Epoch 181, loss: 2.292525\n",
      "Epoch 182, loss: 2.292475\n",
      "Epoch 183, loss: 2.292425\n",
      "Epoch 184, loss: 2.292375\n",
      "Epoch 185, loss: 2.292324\n",
      "Epoch 186, loss: 2.292275\n",
      "Epoch 187, loss: 2.292225\n",
      "Epoch 188, loss: 2.292174\n",
      "Epoch 189, loss: 2.292124\n",
      "Epoch 190, loss: 2.292074\n",
      "Epoch 191, loss: 2.292025\n",
      "Epoch 192, loss: 2.291974\n",
      "Epoch 193, loss: 2.291924\n",
      "Epoch 194, loss: 2.291875\n",
      "Epoch 195, loss: 2.291825\n",
      "Epoch 196, loss: 2.291775\n",
      "Epoch 197, loss: 2.291726\n",
      "Epoch 198, loss: 2.291676\n",
      "Epoch 199, loss: 2.291626\n",
      "Epoch 200, loss: 2.291576\n",
      "Epoch 201, loss: 2.291526\n",
      "Epoch 202, loss: 2.291477\n",
      "Epoch 203, loss: 2.291427\n",
      "Epoch 204, loss: 2.291378\n",
      "Epoch 205, loss: 2.291328\n",
      "Epoch 206, loss: 2.291278\n",
      "Epoch 207, loss: 2.291229\n",
      "Epoch 208, loss: 2.291179\n",
      "Epoch 209, loss: 2.291129\n",
      "Epoch 210, loss: 2.291080\n",
      "Epoch 211, loss: 2.291031\n",
      "Epoch 212, loss: 2.290981\n",
      "Epoch 213, loss: 2.290932\n",
      "Epoch 214, loss: 2.290883\n",
      "Epoch 215, loss: 2.290833\n",
      "Epoch 216, loss: 2.290785\n",
      "Epoch 217, loss: 2.290735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218, loss: 2.290685\n",
      "Epoch 219, loss: 2.290636\n",
      "Epoch 220, loss: 2.290587\n",
      "Epoch 221, loss: 2.290538\n",
      "Epoch 222, loss: 2.290489\n",
      "Epoch 223, loss: 2.290440\n",
      "Epoch 224, loss: 2.290391\n",
      "Epoch 225, loss: 2.290342\n",
      "Epoch 226, loss: 2.290293\n",
      "Epoch 227, loss: 2.290244\n",
      "Epoch 228, loss: 2.290195\n",
      "Epoch 229, loss: 2.290146\n",
      "Epoch 230, loss: 2.290097\n",
      "Epoch 231, loss: 2.290048\n",
      "Epoch 232, loss: 2.289999\n",
      "Epoch 233, loss: 2.289951\n",
      "Epoch 234, loss: 2.289901\n",
      "Epoch 235, loss: 2.289853\n",
      "Epoch 236, loss: 2.289803\n",
      "Epoch 237, loss: 2.289755\n",
      "Epoch 238, loss: 2.289707\n",
      "Epoch 239, loss: 2.289657\n",
      "Epoch 240, loss: 2.289609\n",
      "Epoch 241, loss: 2.289561\n",
      "Epoch 242, loss: 2.289512\n",
      "Epoch 243, loss: 2.289463\n",
      "Epoch 244, loss: 2.289415\n",
      "Epoch 245, loss: 2.289366\n",
      "Epoch 246, loss: 2.289317\n",
      "Epoch 247, loss: 2.289269\n",
      "Epoch 248, loss: 2.289221\n",
      "Epoch 249, loss: 2.289172\n",
      "Epoch 250, loss: 2.289123\n",
      "Epoch 251, loss: 2.289075\n",
      "Epoch 252, loss: 2.289026\n",
      "Epoch 253, loss: 2.288978\n",
      "Epoch 254, loss: 2.288930\n",
      "Epoch 255, loss: 2.288881\n",
      "Epoch 256, loss: 2.288833\n",
      "Epoch 257, loss: 2.288785\n",
      "Epoch 258, loss: 2.288737\n",
      "Epoch 259, loss: 2.288689\n",
      "Epoch 260, loss: 2.288640\n",
      "Epoch 261, loss: 2.288592\n",
      "Epoch 262, loss: 2.288544\n",
      "Epoch 263, loss: 2.288496\n",
      "Epoch 264, loss: 2.288448\n",
      "Epoch 265, loss: 2.288400\n",
      "Epoch 266, loss: 2.288352\n",
      "Epoch 267, loss: 2.288304\n",
      "Epoch 268, loss: 2.288255\n",
      "Epoch 269, loss: 2.288208\n",
      "Epoch 270, loss: 2.288159\n",
      "Epoch 271, loss: 2.288111\n",
      "Epoch 272, loss: 2.288064\n",
      "Epoch 273, loss: 2.288016\n",
      "Epoch 274, loss: 2.287968\n",
      "Epoch 275, loss: 2.287920\n",
      "Epoch 276, loss: 2.287872\n",
      "Epoch 277, loss: 2.287825\n",
      "Epoch 278, loss: 2.287777\n",
      "Epoch 279, loss: 2.287729\n",
      "Epoch 280, loss: 2.287681\n",
      "Epoch 281, loss: 2.287633\n",
      "Epoch 282, loss: 2.287586\n",
      "Epoch 283, loss: 2.287538\n",
      "Epoch 284, loss: 2.287491\n",
      "Epoch 285, loss: 2.287443\n",
      "Epoch 286, loss: 2.287395\n",
      "Epoch 287, loss: 2.287348\n",
      "Epoch 288, loss: 2.287300\n",
      "Epoch 289, loss: 2.287253\n",
      "Epoch 290, loss: 2.287205\n",
      "Epoch 291, loss: 2.287158\n",
      "Epoch 292, loss: 2.287111\n",
      "Epoch 293, loss: 2.287063\n",
      "Epoch 294, loss: 2.287016\n",
      "Epoch 295, loss: 2.286968\n",
      "Epoch 296, loss: 2.286920\n",
      "Epoch 297, loss: 2.286873\n",
      "Epoch 298, loss: 2.286826\n",
      "Epoch 299, loss: 2.286779\n",
      "Epoch 300, loss: 2.286731\n",
      "Epoch 301, loss: 2.286684\n",
      "Epoch 302, loss: 2.286637\n",
      "Epoch 303, loss: 2.286590\n",
      "Epoch 304, loss: 2.286542\n",
      "Epoch 305, loss: 2.286495\n",
      "Epoch 306, loss: 2.286448\n",
      "Epoch 307, loss: 2.286401\n",
      "Epoch 308, loss: 2.286354\n",
      "Epoch 309, loss: 2.286307\n",
      "Epoch 310, loss: 2.286260\n",
      "Epoch 311, loss: 2.286213\n",
      "Epoch 312, loss: 2.286165\n",
      "Epoch 313, loss: 2.286118\n",
      "Epoch 314, loss: 2.286072\n",
      "Epoch 315, loss: 2.286025\n",
      "Epoch 316, loss: 2.285978\n",
      "Epoch 317, loss: 2.285931\n",
      "Epoch 318, loss: 2.285884\n",
      "Epoch 319, loss: 2.285837\n",
      "Epoch 320, loss: 2.285790\n",
      "Epoch 321, loss: 2.285743\n",
      "Epoch 322, loss: 2.285697\n",
      "Epoch 323, loss: 2.285650\n",
      "Epoch 324, loss: 2.285603\n",
      "Epoch 325, loss: 2.285556\n",
      "Epoch 326, loss: 2.285510\n",
      "Epoch 327, loss: 2.285463\n",
      "Epoch 328, loss: 2.285417\n",
      "Epoch 329, loss: 2.285370\n",
      "Epoch 330, loss: 2.285324\n",
      "Epoch 331, loss: 2.285277\n",
      "Epoch 332, loss: 2.285230\n",
      "Epoch 333, loss: 2.285183\n",
      "Epoch 334, loss: 2.285138\n",
      "Epoch 335, loss: 2.285090\n",
      "Epoch 336, loss: 2.285044\n",
      "Epoch 337, loss: 2.284998\n",
      "Epoch 338, loss: 2.284951\n",
      "Epoch 339, loss: 2.284905\n",
      "Epoch 340, loss: 2.284859\n",
      "Epoch 341, loss: 2.284812\n",
      "Epoch 342, loss: 2.284766\n",
      "Epoch 343, loss: 2.284719\n",
      "Epoch 344, loss: 2.284673\n",
      "Epoch 345, loss: 2.284627\n",
      "Epoch 346, loss: 2.284581\n",
      "Epoch 347, loss: 2.284534\n",
      "Epoch 348, loss: 2.284488\n",
      "Epoch 349, loss: 2.284442\n",
      "Epoch 350, loss: 2.284396\n",
      "Epoch 351, loss: 2.284349\n",
      "Epoch 352, loss: 2.284304\n",
      "Epoch 353, loss: 2.284257\n",
      "Epoch 354, loss: 2.284212\n",
      "Epoch 355, loss: 2.284165\n",
      "Epoch 356, loss: 2.284119\n",
      "Epoch 357, loss: 2.284073\n",
      "Epoch 358, loss: 2.284027\n",
      "Epoch 359, loss: 2.283981\n",
      "Epoch 360, loss: 2.283935\n",
      "Epoch 361, loss: 2.283889\n",
      "Epoch 362, loss: 2.283843\n",
      "Epoch 363, loss: 2.283797\n",
      "Epoch 364, loss: 2.283751\n",
      "Epoch 365, loss: 2.283705\n",
      "Epoch 366, loss: 2.283659\n",
      "Epoch 367, loss: 2.283614\n",
      "Epoch 368, loss: 2.283568\n",
      "Epoch 369, loss: 2.283522\n",
      "Epoch 370, loss: 2.283476\n",
      "Epoch 371, loss: 2.283431\n",
      "Epoch 372, loss: 2.283385\n",
      "Epoch 373, loss: 2.283339\n",
      "Epoch 374, loss: 2.283294\n",
      "Epoch 375, loss: 2.283248\n",
      "Epoch 376, loss: 2.283202\n",
      "Epoch 377, loss: 2.283157\n",
      "Epoch 378, loss: 2.283111\n",
      "Epoch 379, loss: 2.283066\n",
      "Epoch 380, loss: 2.283020\n",
      "Epoch 381, loss: 2.282975\n",
      "Epoch 382, loss: 2.282929\n",
      "Epoch 383, loss: 2.282884\n",
      "Epoch 384, loss: 2.282838\n",
      "Epoch 385, loss: 2.282793\n",
      "Epoch 386, loss: 2.282747\n",
      "Epoch 387, loss: 2.282701\n",
      "Epoch 388, loss: 2.282656\n",
      "Epoch 389, loss: 2.282611\n",
      "Epoch 390, loss: 2.282566\n",
      "Epoch 391, loss: 2.282520\n",
      "Epoch 392, loss: 2.282475\n",
      "Epoch 393, loss: 2.282430\n",
      "Epoch 394, loss: 2.282385\n",
      "Epoch 395, loss: 2.282340\n",
      "Epoch 396, loss: 2.282294\n",
      "Epoch 397, loss: 2.282249\n",
      "Epoch 398, loss: 2.282204\n",
      "Epoch 399, loss: 2.282159\n",
      "Epoch 400, loss: 2.282114\n",
      "Epoch 401, loss: 2.282068\n",
      "Epoch 402, loss: 2.282023\n",
      "Epoch 403, loss: 2.281978\n",
      "Epoch 404, loss: 2.281933\n",
      "Epoch 405, loss: 2.281888\n",
      "Epoch 406, loss: 2.281843\n",
      "Epoch 407, loss: 2.281798\n",
      "Epoch 408, loss: 2.281753\n",
      "Epoch 409, loss: 2.281708\n",
      "Epoch 410, loss: 2.281663\n",
      "Epoch 411, loss: 2.281619\n",
      "Epoch 412, loss: 2.281574\n",
      "Epoch 413, loss: 2.281529\n",
      "Epoch 414, loss: 2.281484\n",
      "Epoch 415, loss: 2.281439\n",
      "Epoch 416, loss: 2.281394\n",
      "Epoch 417, loss: 2.281349\n",
      "Epoch 418, loss: 2.281304\n",
      "Epoch 419, loss: 2.281260\n",
      "Epoch 420, loss: 2.281215\n",
      "Epoch 421, loss: 2.281170\n",
      "Epoch 422, loss: 2.281126\n",
      "Epoch 423, loss: 2.281081\n",
      "Epoch 424, loss: 2.281036\n",
      "Epoch 425, loss: 2.280992\n",
      "Epoch 426, loss: 2.280947\n",
      "Epoch 427, loss: 2.280903\n",
      "Epoch 428, loss: 2.280858\n",
      "Epoch 429, loss: 2.280814\n",
      "Epoch 430, loss: 2.280769\n",
      "Epoch 431, loss: 2.280725\n",
      "Epoch 432, loss: 2.280681\n",
      "Epoch 433, loss: 2.280636\n",
      "Epoch 434, loss: 2.280591\n",
      "Epoch 435, loss: 2.280547\n",
      "Epoch 436, loss: 2.280503\n",
      "Epoch 437, loss: 2.280458\n",
      "Epoch 438, loss: 2.280414\n",
      "Epoch 439, loss: 2.280369\n",
      "Epoch 440, loss: 2.280325\n",
      "Epoch 441, loss: 2.280280\n",
      "Epoch 442, loss: 2.280236\n",
      "Epoch 443, loss: 2.280192\n",
      "Epoch 444, loss: 2.280148\n",
      "Epoch 445, loss: 2.280104\n",
      "Epoch 446, loss: 2.280060\n",
      "Epoch 447, loss: 2.280015\n",
      "Epoch 448, loss: 2.279971\n",
      "Epoch 449, loss: 2.279927\n",
      "Epoch 450, loss: 2.279883\n",
      "Epoch 451, loss: 2.279838\n",
      "Epoch 452, loss: 2.279795\n",
      "Epoch 453, loss: 2.279751\n",
      "Epoch 454, loss: 2.279706\n",
      "Epoch 455, loss: 2.279663\n",
      "Epoch 456, loss: 2.279618\n",
      "Epoch 457, loss: 2.279575\n",
      "Epoch 458, loss: 2.279530\n",
      "Epoch 459, loss: 2.279486\n",
      "Epoch 460, loss: 2.279443\n",
      "Epoch 461, loss: 2.279398\n",
      "Epoch 462, loss: 2.279355\n",
      "Epoch 463, loss: 2.279311\n",
      "Epoch 464, loss: 2.279267\n",
      "Epoch 465, loss: 2.279223\n",
      "Epoch 466, loss: 2.279179\n",
      "Epoch 467, loss: 2.279135\n",
      "Epoch 468, loss: 2.279092\n",
      "Epoch 469, loss: 2.279048\n",
      "Epoch 470, loss: 2.279004\n",
      "Epoch 471, loss: 2.278960\n",
      "Epoch 472, loss: 2.278917\n",
      "Epoch 473, loss: 2.278873\n",
      "Epoch 474, loss: 2.278829\n",
      "Epoch 475, loss: 2.278786\n",
      "Epoch 476, loss: 2.278742\n",
      "Epoch 477, loss: 2.278698\n",
      "Epoch 478, loss: 2.278656\n",
      "Epoch 479, loss: 2.278611\n",
      "Epoch 480, loss: 2.278568\n",
      "Epoch 481, loss: 2.278524\n",
      "Epoch 482, loss: 2.278480\n",
      "Epoch 483, loss: 2.278437\n",
      "Epoch 484, loss: 2.278394\n",
      "Epoch 485, loss: 2.278350\n",
      "Epoch 486, loss: 2.278308\n",
      "Epoch 487, loss: 2.278264\n",
      "Epoch 488, loss: 2.278220\n",
      "Epoch 489, loss: 2.278177\n",
      "Epoch 490, loss: 2.278134\n",
      "Epoch 491, loss: 2.278090\n",
      "Epoch 492, loss: 2.278047\n",
      "Epoch 493, loss: 2.278004\n",
      "Epoch 494, loss: 2.277961\n",
      "Epoch 495, loss: 2.277917\n",
      "Epoch 496, loss: 2.277873\n",
      "Epoch 497, loss: 2.277830\n",
      "Epoch 498, loss: 2.277788\n",
      "Epoch 499, loss: 2.277744\n",
      "Accuracy after training for 500 epochs:  learning_rate: 1e-05 reg_strengths 0 -- 0.057\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "batch_s = 50\n",
    "\n",
    "learning_rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [0]#, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "\n",
    "for l in learning_rates:\n",
    "    for r in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        loss_history = classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=l, batch_size=batch_s, reg=r)\n",
    "        #print(loss_history)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        print(\"Accuracy after training for\", num_epochs, \"epochs: \", \"learning_rate:\", l, \"reg_strengths\", r, \"--\", accuracy)\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "#print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-51d73b5f6183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Linear softmax classifier test set accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
